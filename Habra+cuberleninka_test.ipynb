{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[33mWARNING: You are using pip version 22.0.4; however, version 22.1 is available.\r\n",
      "You should consider upgrading via the '/home/paul/anaconda3/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -q langdetect jsonlines"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from bertopic import BERTopic\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from octis.evaluation_metrics.diversity_metrics import TopicDiversity\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import umap\n",
    "import optuna\n",
    "import hdbscan\n",
    "from tqdm.notebook import tqdm\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from pylatexenc.latex2text import LatexNodes2Text\n",
    "from razdel import tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from natasha import Doc, MorphVocab, Segmenter, NewsEmbedding, NewsMorphTagger\n",
    "from langdetect import detect\n",
    "\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "tqdm.pandas()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def get_crps(txts: list):\n",
    "    ret = []\n",
    "    for el in txts:\n",
    "        ret.append(el.split())\n",
    "    return ret\n",
    "\n",
    "\n",
    "def get_dict(data):\n",
    "    return dict(zip(data['Topic'].tolist(), [el.split('_')[1:] for el in data['Name'].tolist()]))\n",
    "\n",
    "\n",
    "def transform_topics(lst, dct):\n",
    "    ret = []\n",
    "    for el in lst:\n",
    "        ret.append(dct[el])\n",
    "    return ret\n",
    "\n",
    "\n",
    "def compute_coherence(topic_model_, name_c, data, topics_, topn):\n",
    "    cleaned_docs = topic_model_._preprocess_text(data)\n",
    "\n",
    "    # Extract vectorizer and tokenizer from BERTopic\n",
    "    vectorizer = topic_model_.vectorizer_model\n",
    "    tokenizer = vectorizer.build_tokenizer()\n",
    "\n",
    "    # Extract features for Topic Coherence evaluation\n",
    "    words = vectorizer.get_feature_names()\n",
    "    tokens = [tokenizer(doc) for doc in cleaned_docs]\n",
    "    dictionary = corpora.Dictionary(tokens)\n",
    "    corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "    topic_words = [[words for words, _ in topic_model_.get_topic(topic) if words!='']\n",
    "                   for topic in range(len(set(topics_))-1)]\n",
    "\n",
    "    # Evaluate\n",
    "    coherence_model = CoherenceModel(topics=topic_words,\n",
    "                                     texts=tokens,\n",
    "                                     corpus=corpus,\n",
    "                                     dictionary=dictionary,\n",
    "                                     coherence=name_c, topn=topn)\n",
    "    coherence = coherence_model.get_coherence()\n",
    "    return coherence\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_metrics(topics_, topic_model_, data, top_k=5):\n",
    "    try:\n",
    "        output = {'topics' : [[words for words, _ in topic_model_.get_topic(topic)]for topic in range(len(set\n",
    "                                                                                                          (topics_[0]))-1)]}\n",
    "        topic_diversity = TopicDiversity(topk=top_k)\n",
    "        topic_diversity_score = topic_diversity.score(output)\n",
    "    except:\n",
    "        topic_diversity_score = None\n",
    "\n",
    "    npmi_score = compute_coherence(topic_model_,'c_npmi', data, topics_[0], top_k)\n",
    "    cv_score = compute_coherence(topic_model_,'c_v', data, topics_[0], top_k)\n",
    "\n",
    "    return topic_diversity_score, npmi_score, cv_score\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def objective(trial):\n",
    "    torch.cuda.empty_cache()\n",
    "    tnw = trial.suggest_int(\"top_n_words\", 10, 30, log=True)\n",
    "    ngr = trial.suggest_int(\"n_gram_range\", 1, 3, log=True)\n",
    "    mts = trial.suggest_int(\"min_topic_size\", 5, 50, log=True)\n",
    "\n",
    "    min_dist = trial.suggest_float(\"min_dist\", 0.000001, 1, log=True)\n",
    "    n_neigh = trial.suggest_int(\"n_neighbors\", 2, 100, log=True)\n",
    "    n_comp = trial.suggest_int(\"n_components\", 10, 250, log=True)\n",
    "    umap_model = umap.UMAP(\n",
    "        n_neighbors=n_neigh,\n",
    "        min_dist=min_dist,\n",
    "        n_components=n_comp,\n",
    "        random_state=42,\n",
    "    )\n",
    "    # cse = trial.suggest_float(\"cluster_selection_epsilon\", 0.0001, 10, log=True)\n",
    "    # mcs = trial.suggest_int(\"min_cluster_size\", 2, 100, log=True)\n",
    "    ms = trial.suggest_int(\"min_samples\", 2, 40, log=True)\n",
    "    hdbscan_model = hdbscan.HDBSCAN(min_samples=ms)\n",
    "    topic_model = BERTopic(embedding_model=sentence_model, top_n_words=tnw, n_gram_range=(1, ngr), min_topic_size=mts,\n",
    "                           umap_model=umap_model, hdbscan_model=hdbscan_model, verbose=False)\n",
    "\n",
    "\n",
    "    topics = topic_model.fit_transform(X_train)\n",
    "    return compute_metrics(topics, topic_model, X_train)[1]\n",
    "\n",
    "def extract_pars(dct):\n",
    "    tmp = dict()\n",
    "    tmp['min_dist'] = dct['min_dist']\n",
    "    tmp['n_neighbors'] = dct['n_neighbors']\n",
    "    tmp['n_components'] = dct['n_components']\n",
    "    tmp_1 = dict()\n",
    "    # tmp_1['cluster_selection_epsilon'] = dct['cluster_selection_epsilon']\n",
    "    # tmp_1['min_cluster_size'] = dct['min_cluster_size']\n",
    "    tmp_1['min_samples'] = dct['min_samples']\n",
    "    tmp_2 = dict()\n",
    "    tmp_2['top_n_words'] = dct['top_n_words']\n",
    "    tmp_2['n_gram_range'] = (1, dct['n_gram_range'])\n",
    "    tmp_2['min_topic_size'] = dct['min_topic_size']\n",
    "    return tmp, tmp_1, tmp_2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "/home/paul/Documents/ВКР/ru_kw_eval_datasets-master/data/cyberleninka_0.jsonlines.zip"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "from os import walk\n",
    "filenames = next(walk('ru_kw_eval_datasets-master/data/'), (None, None, []))[2]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "['habrahabr_0.jsonlines.zip',\n 'russia_today_5.jsonlines.zip',\n 'russia_today_0.jsonlines.zip',\n 'cyberleninka_4.jsonlines.zip',\n 'cyberleninka_1.jsonlines.zip',\n 'cyberleninka_3.jsonlines.zip',\n 'russia_today_2.jsonlines.zip',\n 'ng_0.jsonlines.zip',\n 'habrahabr_2.jsonlines.zip',\n 'russia_today_7.jsonlines.zip',\n 'ng_1.jsonlines.zip',\n 'habrahabr_3.jsonlines.zip',\n 'russia_today_6.jsonlines.zip',\n 'russia_today_3.jsonlines.zip',\n 'cyberleninka_2.jsonlines.zip',\n 'cyberleninka_0.jsonlines.zip',\n 'russia_today_1.jsonlines.zip',\n 'habrahabr_1.jsonlines.zip',\n 'russia_today_4.jsonlines.zip']"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "import zipfile\n",
    "for fl in filenames:\n",
    "    tmp = 'ru_kw_eval_datasets-master/data/' + fl\n",
    "    with zipfile.ZipFile(tmp, 'r') as zip_ref:\n",
    "        zip_ref.extractall('ru_data/')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Habr"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "3990"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jsonlines\n",
    "habr = []\n",
    "for fl in [f'ru_data/habrahabr_{i}.jsonlines' for i in range(4)]:\n",
    "    with jsonlines.open(fl, 'r') as jsonl_f:\n",
    "        habr += [obj for obj in jsonl_f]\n",
    "len(habr)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "{'content': ' MassTransit это open source библиотека, разработанная на языке C# для .NET платформы, упрощающая работу с шиной данных, которая используется при построении распределенных приложений и реализации SOA (service oriented architecture). \\r\\nВ качестве message broker могут выступать RabbitMq, Azure Service Bus или In-Memory менеджер (в случае с In-Memory область видимости ограничивается процессом, в котором проинициализирован экземпляр). Содержание: Команды и события Команды \\r\\n События \\r\\n \\r\\n Контракты сообщений \\r\\n Роутинг Exchange \\r\\n Формат сообщения \\r\\n \\r\\n Консьюмеры (Consumer) \\r\\n Конфигурация контейнера DI \\r\\n Наблюдатели (Observer) \\r\\n Новое в MassTransit 3.0 \\r\\n Заключение \\r\\n Опрос: А какую .NET библиотеку используете вы? \\r\\n Команды и события \\r\\nВ библиотеке заложено 2 основных типа сообщений: команды и события. Команды \\r\\nСигнализируют о необходимости выполнить некое действие. Для наиболее содержательного наименования команды желательно использовать структуру глагол + существительное: \\r\\nEstimateConnection, SendSms, NotifyCustomerOrderProcessed. \\r\\nРабота с командами осуществляется с помощью метода Send (интерфейса ISendEndpoint ) и указания получателя endpoint (очереди): Отправка команды private static async Task SendSmsCommand(IBus busControl)\\n{\\n var command = new SendSmsCommand\\n {\\n CommandId = Guid.NewGuid(),\\n PhoneNumber = \"89031112233\",\\n Message = \"Thank you for your reply\"\\n };\\n\\n var endpoint = await busControl.GetSendEndpoint(AppSettings.CommandEndpoint);\\n await endpoint.Send(command);\\n}\\n События \\r\\nСигнализируют о случившемся событии, которое может быть интересно некоему набору подписчиков (паттерн Observer), которые на эти события реагируют, например: ConnectionEstimated, CallTerminated, SmsSent, CustomerNotified. \\r\\nРабота с событиями осуществляется с помощью метода Publish (интерфейса IPublishEndpoint ). \\r\\nВ терминологии заложено и основное различие этих типов сообщений — команда доставляется единственному исполнителю (дабы избежать дублирования выполнения): \\r\\nИзображение из статьи MassTransit Send vs. Publish \\r\\nВ то время как событие ориентировано на оповещение n-подписчиков, каждый из которых реагирует на случившееся событие по-своему. \\r\\nИзображение из статьи MassTransit Send vs. Publish \\r\\nИными словами, при запущенных n-консьюмеров (от англ. consumer — потребитель, обработчик), обрабатывающих команду, после её публикации только один из них получит сообщение о ней, в то время как сообщение о событии получит каждый. Контракты сообщений \\r\\nСогласно документации MassTransit, при объявлении контрактов сообщений рекомендуется прибегать к интерфейсам: Контракт : команда на отправку СМС сообщения public interface ISendSms {\\n\\tGuid CommandId { get; }\\n\\tstring PhoneNumber { get; }\\n\\tstring Message { get; }\\n}\\n \\r\\nКак упоминалось ранее, отправка команд должна осуществляться исключительно с помощью метода Send (интерфейса IBus) и указания адресата (endpoint). Контракт: событие об успешной отправке СМС сообщения public interface ISmsSent {\\n\\tGuid EventId { get; }\\n\\tDateTime SentAtUtc { get; }\\t\\n}\\n \\r\\nСобытия отправляются с помощью метода Publish. Роутинг \\r\\nКак распределение сообщений по exchange, так и выбор консьюмеров (о них в этой статье будет рассказано чуть позже) для обработки базируются на runtime типах этих сообщений,- в наименовании используются namespace и имя типа, в случае с generic имя родительского типа и перечень аргументов. Exchange \\r\\nПри конфигурации receive endpoint ‘a (подключении ранее зарегистрированных консьюмеров) в случае использования в качестве канала доставки сообщений RabbitMq на основании указанных к обработке консьюмерами типов сообщений формируются наименования требуемых exchange, в которые затем эти самые сообщения и будут помещаться. \\r\\nАналогичные действия на этапе конфигурации send endpoint ‘a выполняются и для команд, для отправки которых также требуются собственные exchange. \\r\\nНа изображении можно увидеть созданные в рамках моего сценария exchange: \\r\\nВ том случае, если конфигурируя receive endpoint мы указываем наименование очереди: cfg.ReceiveEndpoint(host, \"play_with_masstransit_queue\", e =&gt; e.LoadFrom(container));\\n \\r\\nто в привязках exchange сообщений можно будет увидеть следующую картину: \\r\\nИтоговый путь сообщения, тип которого имплементирует ISmsEvent, будет иметь следующий вид: \\r\\nЕсли же конфигурация осуществляется без указания очереди: cfg.ReceiveEndpoint(host, e=&gt; e.LoadFrom(container));\\n \\r\\nТо имена для последнего exchange и очереди формируются автоматически, а по завершению работы они будут удалены: Формат сообщения \\r\\nГоворя о формате сообщения, хотелось бы подробнее остановиться на наименовании (или messageType). За его формирование (заголовков urn:message:) ответственна функция GetUrnForType(Type type) . Для примера я добавил для команды ISendSms наследование от ICommand и generic тип: Контракт : команда на отправку СМС сообщения ICommand&lt;string&gt; public interface ICommand&lt;T&gt;\\n{\\n}\\n\\npublic interface ISendSms&lt;T&gt; : ICommand&lt;T&gt;\\n{\\n Guid CommandId { get; }\\n string PhoneNumber { get; }\\n string Message { get; }\\n}\\n\\nclass SendSmsCommand : ISendSms&lt;string&gt;\\n{\\n public Guid CommandId { get; set; }\\n public string PhoneNumber { get; set; }\\n public string Message { get; set; }\\n}\\n \\r\\nСформированное сообщение в таком случае будет содержать следующее значение в поле messageType (на основании которого после получения сообщения затем и выбирается ответственный за обработку консьюмер): \"messageType\": [\\n \"urn:message:PlayWithMassTransit30.Extension:SendSmsCommand\",\\n \"urn:message:PlayWithMassTransit30.Contract.Command:ISendSms[[System:String]]\",\\n \"urn:message:PlayWithMassTransit30.Contract.Command:ICommand[[System:String]]\"\\n]\\n \\r\\nКроме messageType сообщение содержит информацию о host, которым оно было отправлено: \"host\": {\\n \"machineName\": \"DESKTOP-SI9OHUR\",\\n \"processName\": \"PlayWithMassTransit30.vshost\",\\n \"processId\": 1092,\\n \"assembly\": \"PlayWithMassTransit30\",\\n \"assemblyVersion\": \"1.0.0.0\",\\n \"frameworkVersion\": \"4.0.30319.42000\",\\n \"massTransitVersion\": \"3.4.1.808\",\\n \"operatingSystemVersion\": \"Microsoft Windows NT 6.2.9200.0\"\\n}\\n \\r\\nЗначимую часть payload: \"message\": {\\n \"commandId\": \"7388f663-82dc-403a-8bf9-8952f2ff262e\",\\n \"phoneNumber\": \"89031112233\",\\n \"message\": \"Thank you for your reply\"\\n}\\n \\r\\nи иные служебные поля и заголовки. Консьюмеры (Consumer) \\r\\nКонсьюмер — это класс, который обрабатывает один или несколько типов сообщений, указываемых при объявлении в наследовании интерфейса IConsumer, где T это тип обрабатываемого данным консьюмером сообщения. \\r\\nПример консьюмера, обрабатывающего команду ISendSms и публикующего событие ISmsSent: SendSmsConsumer: обработчик команды на отправку сообщения public class SendSmsConsumer : IConsumer&lt;ISendSms&lt;string&gt;&gt;\\n{\\n public SendSmsConsumer(IBusControl busControl)\\n {\\n _busControl = busControl;\\n }\\n\\n public async Task Consume(ConsumeContext&lt;ISendSms&lt;string&gt;&gt; context)\\n {\\n var message = context.Message;\\n\\n Console.WriteLine($\"[IConsumer&lt;ISendSms&gt;] Send sms command consumed\");\\n Console.WriteLine($\"[IConsumer&lt;ISendSms&gt;] CommandId: {message.CommandId}\");\\n Console.WriteLine($\"[IConsumer&lt;ISendSms&gt;] Phone number: {message.PhoneNumber}\");\\n Console.WriteLine($\"[IConsumer&lt;ISendSms&gt;] Message: {message.Message}\");\\n\\n Console.Write(Environment.NewLine);\\n Console.WriteLine(\"Публикация события: Смс сообщение отправлено\");\\n await _busControl.SmsSent(DateTime.UtcNow);\\n }\\n\\n private readonly IBus _busControl;\\n}\\n \\r\\nПосле того, как мы получили команду на отправку смс сообщения и выполнили требуемые действия, мы формируем и отправляем событие о том, что смс доставлено. \\r\\nКод отправки сообщений я вынес в отдельный Extension класс над IBusControl, там же находится и имплементация самих сообщений: Методы расширения над IBus для инкапсуляции логики межсистемного взаимодействия public static class BusExtensions\\n{\\n /// &lt;summary&gt;\\n /// Отправка смс сообщения\\n /// &lt;/summary&gt;\\n /// &lt;param name=\"bus\"&gt;&lt;/param&gt;\\n /// &lt;param name=\"host\"&gt;&lt;/param&gt;\\n /// &lt;param name=\"phoneNumber\"&gt;&lt;/param&gt;\\n /// &lt;param name=\"message\"&gt;&lt;/param&gt;\\n /// &lt;returns&gt;&lt;/returns&gt;\\n public static async Task SendSms(\\n this IBus bus, Uri host, string phoneNumber, string message\\n )\\n {\\n var command = new SendSmsCommand\\n {\\n CommandId = Guid.NewGuid(),\\n PhoneNumber = phoneNumber,\\n Message = message\\n };\\n\\n await bus.SendCommand(host, command);\\n }\\n\\n /// &lt;summary&gt;\\n /// Публикация события об отправке смс сообщения\\n /// &lt;/summary&gt;\\n /// &lt;param name=\"bus\"&gt;&lt;/param&gt;\\n /// &lt;param name=\"sentAtUtc\"&gt;&lt;/param&gt;\\n /// &lt;returns&gt;&lt;/returns&gt;\\n public static async Task SmsSent(\\n this IBus bus, DateTime sentAtUtc\\n )\\n {\\n var @event = new SmsSentEvent\\n {\\n EventId = Guid.NewGuid(),\\n SentAtUtc = sentAtUtc\\n };\\n\\n await bus.PublishEvent(@event);\\n }\\n\\n /// &lt;summary&gt;\\n /// Отправка команды\\n /// &lt;/summary&gt;\\n /// &lt;typeparam name=\"T\"&gt;&lt;/typeparam&gt;\\n /// &lt;param name=\"bus\"&gt;&lt;/param&gt;\\n /// &lt;param name=\"address\"&gt;&lt;/param&gt;\\n /// &lt;param name=\"command\"&gt;&lt;/param&gt;\\n /// &lt;returns&gt;&lt;/returns&gt;\\n private static async Task SendCommand&lt;T&gt;(this IBus bus, Uri address, T command) where T : class\\n {\\n var endpoint = await bus.GetSendEndpoint(address);\\n await endpoint.Send(command);\\n }\\n\\n /// &lt;summary&gt;\\n /// Публикация события\\n /// &lt;/summary&gt;\\n /// &lt;typeparam name=\"T\"&gt;&lt;/typeparam&gt;\\n /// &lt;param name=\"bus\"&gt;&lt;/param&gt;\\n /// &lt;param name=\"message\"&gt;&lt;/param&gt;\\n /// &lt;returns&gt;&lt;/returns&gt;\\n private static async Task PublishEvent&lt;T&gt;(this IBus bus, T message) where T : class\\n {\\n await bus.Publish(message);\\n }\\n}\\n\\nclass SendSmsCommand : ISendSms&lt;string&gt;\\n{\\n public Guid CommandId { get; set; }\\n public string PhoneNumber { get; set; }\\n public string Message { get; set; }\\n}\\n\\nclass SmsSentEvent : ISmsSent\\n{\\n public Guid EventId { get; set; }\\n public DateTime SentAtUtc { get; set; }\\n}\\n \\r\\nНа мой взгляд, данное решение вполне удачно позволяет отделить код бизнес-логики от деталей реализации межсистемного (компонентного) взаимодействия и инкапсулировать их в одном месте. Конфигурация контейнера DI \\r\\nНа данный момент MassTransit предоставляет возможность использовать следующие популярные контейнеры : Autofac; \\r\\n Ninject; \\r\\n StructureMap; \\r\\n Unity; \\r\\n Castle Windsor. \\r\\n \\r\\nВ случае с UnityContainer потребуется установить nuget package MassTransit.Unity, после чего станет доступен метод расширения LoadFrom: public static class UnityExtensions\\n{\\n public static void LoadFrom(this IReceiveEndpointConfigurator configurator, IUnityContainer container);\\n}\\n \\r\\nПример использования выглядит следующим образом: Конфигурация IBusControl с помощью UnityContainer public static IBusControl GetConfiguredFactory(IUnityContainer container)\\n{\\n if (container == null)\\n {\\n throw new ArgumentNullException(nameof(container));\\n }\\n\\n var control = Bus.Factory.CreateUsingRabbitMq(cfg =&gt; {\\n var host = cfg.Host(AppSettings.Host, h =&gt; { });\\n\\n\\n // cfg.ReceiveEndpoint(host, e =&gt; e.LoadFrom(container));\\n cfg.ReceiveEndpoint(host, \"play_with_masstransit_queue\", e =&gt; e.LoadFrom(container));\\n });\\n\\n control.ConnectConsumeObserver(new ConsumeObserver());\\n control.ConnectReceiveObserver(new ReceiveObserver());\\n control.ConnectConsumeMessageObserver(new ConsumeObserverSendSmsCommand());\\n control.ConnectSendObserver(new SendObserver());\\n control.ConnectPublishObserver(new PublishObserver());\\n\\n return control;\\n}\\n \\r\\nВ качестве срока жизни консьюмеров в контейнере документация предлагает использовать ContainerControlledLifetimeManager(). Наблюдатели (Observer) \\r\\nДля мониторинга процесса обработки сообщений доступно подключение наблюдателей (Observer). Для этого MassTransit предоставляет следующий набор интерфейсов для обработчиков: IReceiveObserver- срабатывает сразу же после получения сообщения и создания RecieveContext; \\r\\n IConsumeObserver — срабатывает после создания ConsumeContext; \\r\\n IConsumeMessageObserver&lt;T&gt; — для наблюдения за сообщениями типа T, в методах которого будет доступно строго-типизированное содержимое сообщения; \\r\\n ISendObserver — для наблюдения за отправляемыми командами; \\r\\n IPublishObserver — для наблюдения за отправляемыми событиями. \\r\\n \\r\\nДля каждого из них интерфейс IBusControl предоставляет собственный метод подключения, выполнение которого должно быть осуществлено непосредственно перед IBusControl.Start(). \\r\\nВ качестве примера далее представлена реализация ConsumeObserver: Реализация IConsumeObsever public class ConsumeObserver : IConsumeObserver\\n{\\n public Task PreConsume&lt;T&gt;(ConsumeContext&lt;T&gt; context) where T : class\\n {\\n Console.WriteLine($\"[ConsumeObserver] PreConsume {context.MessageId}\");\\n return Task.CompletedTask;\\n }\\n\\n public Task PostConsume&lt;T&gt;(ConsumeContext&lt;T&gt; context) where T : class\\n {\\n Console.WriteLine($\"[ConsumeObserver] PostConsume {context.MessageId}\");\\n return Task.CompletedTask;\\n }\\n\\n public Task ConsumeFault&lt;T&gt;(ConsumeContext&lt;T&gt; context, Exception exception) where T : class\\n {\\n Console.WriteLine($\"[ConsumeObserver] ConsumeFault {context.MessageId}\");\\n return Task.CompletedTask;\\n }\\n}\\n \\r\\nЯ не буду приводить код каждого из консьюмеров, т.к. по принципу работы и структуре они схожи. Имплементацию каждого из них можно посмотреть в документации или в исходниках на Github . \\r\\nИтоговый pipeline получения команды на отправку смс сообщения, её обработки и публикации события о её успешном выполнении выглядит следующим образом: Новое в MassTransit 3.0 \\r\\nС изменениями, которые коснулись новой версии библиотеки, вы можете ознакомиться в 2-х обзорных статьях автора библиотеки Chris Patterson’а на страницах его блога: MassTransit 3 API Changes и MassTransit v3 Update . Заключение \\r\\nЗдесь должно было быть сравнение наиболее популярных библиотек для работы с очередями сообщений, однако, я решил оставить это для отдельной статьи. \\r\\nНадеюсь, мне удалось провести для вас поверхностное знакомство с библиотекой MassTransit, за гранью которого ещё остаются такие интересные вещи, как транзакционность, персистентность (интеграция с NHibernate, MondoDb, EntityFramework), планировщик отправки сообщений (интеграция с Quartz), state machine (Automatonymous и Saga), логирование (Log4Net, NLog), многопоточность и многое другое. \\r\\nИсходные коды примеров доступны на Github . Используемые материалы: Документация MassTransit . \\r\\n ',\n 'title': 'Опыт использования MassTransit 3.0',\n 'summary': 'MassTransit это open source библиотека, разработанная на языке C# для .NET платформы, упрощающая работу с шиной данных, которая используется при построении распределенных приложений и реализации...',\n 'url': 'https://habrahabr.ru/post/314080/',\n 'keywords': ['.net', 'rabbitmq', 'masstransit']}"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "habr[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "{'content': ' \\r\\nНа сегодняшний день процедура реализации «failover» в Postgresql является одной из самых простых и интуитивно понятных. Для ее реализации необходимо определиться со сценариями файловера — это залог успешной работы кластера, протестировать его работу. В двух словах — настраивается репликация, чаще всего асинхронная, и в случае отказа текущего мастера, другая нода(standby) становится текущем «мастером», другие ноды standby начинают следовать за новым мастером. \\r\\nНа сегодняшний день repmgr поддерживает сценарий автоматического Failover — autofailover, что позволяет поддерживать кластер в рабочем состоянии после выхода из строя ноды-мастера без мгновенного вмешательства сотрудника, что немаловажно, так как не происходит большого падения UPTIME. Для уведомлений используем telegram. \\r\\n Появилась необходимость в связи с развитием внутренних сервисов реализовать систему хранения БД на Postgresql + репликация + балансировка + failover(отказоустойчивость). Как всегда в интернете вроде бы что то и есть, но всё оно устаревшее или на практике не реализуемое в том виде, в котором оно представлено. Было решено представить данное решение, чтобы в будущем у специалистов, решивших реализовать подобную схему было представление как это делается, и чтобы новичкам было легко это реализовать следуя данной инструкции. Постарались описать все как можно подробней, вникнуть во все нюансы и особенности. \\r\\n Итак, что мы имеем: 5 VM с debian 8,Postgresql 9.6 + repmgr (для управления кластером), балансировка и HA на базе HAPROXY (ПО для обеспечения балансировки и высокой доступности web приложения и баз данных) и легковесного менеджера подключений Pgbouncer, keepalived для миграции ip адреса(VIP) между нодами,5-я witness нода для контроля кластера и предотвращения “split brain” ситуаций, когда не могла быть определена следующая мастер нода после отказа текущего мастера. Уведомления через telegram( без него как без рук). \\r\\n Пропишем ноды /etc/hosts — для удобства, так как в дальнейшем все будет оперировать с доменными именами. файл /etc/hosts 10.1.1.195 - pghost195\\n10.1.1.196 - pghost196\\n10.1.1.197 - pghost197\\n10.1.1.198 - pghost198\\n10.1.1.205 - pghost205\\n \\r\\nVIP 10.1.1.192 — запись, 10.1.1.202 — roundrobin(балансировка/только чтение). Установка Postgresql 9.6 pgbouncer haproxy repmgr \\r\\nСтавим на все ноды Установка Postgresql-9.6 и repmgr debian 8 touch /etc/apt/sources.list.d/pgdg.list\\necho “deb http://apt.postgresql.org/pub/repos/apt/ jessie-pgdg main” &gt; /etc/apt/sources.list.d/pgdg.list\\nwget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | apt-key add -\\napt-get update\\nwget http://ftp.ru.debian.org/debian/pool/main/p/pkg-config/pkg-config_0.28-1_amd64.deb\\ndpkg -i pkg-config_0.28-1_amd64.deb\\napt-get install postgresql-9.6-repmgr libevent-dev -y\\n \\r\\nОтключаем автозапуск Postgresql при старте системы — всеми процессами будет управлять пользователь postgres. Так же это необходимо, для того, чтобы бы не было ситуаций, когда у нас сможет оказаться две мастер-ноды, после восстановления одной после сбоя питания, например. Как отключить автозапуск nano /etc/postgresql/9.6/main/start.conf \\nзаменяем auto на manual\\n \\r\\nТак же будем использовать chkconfig для контроля и управления автозапуска. Установка chkconfig apt-get install chkconfig -y \\n \\r\\nСмотрим автозапуск Скрытый текст /sbin/chkconfig --list\\n \\r\\nОтключаем Скрытый текст update-rc.d postgresql disable\\n \\r\\nСмотрим postgresql теперь Скрытый текст /sbin/chkconfig --list postgresql\\n \\r\\nГотово. Идём далее. Настройка ssh соединения без пароля — между всеми нодами(делаем на всех серверах) \\r\\nНастроим подключения между всеми серверами и к самому себе через пользователя postgres(через пользователя postgres подключается также repmgr). \\r\\nУстановим пакеты, которые нам понадобятся для работы(сразу ставим) Ставим ssh и rsync apt-get install openssh-server rsync -y\\n \\r\\nДля начала установим ему локальный пароль для postgres (сразу проделаем это на всех нодах). Скрытый текст passwd postgres \\n \\r\\nВведем новый пароль. \\r\\nОк. \\r\\nДалее настроим ssh соединение Скрытый текст su postgres\\ncd ~\\nssh-keygen\\n \\r\\nГенерируем ключ — без пароля. \\r\\nСтавим ключ на другие ноды Скрытый текст ssh-copy-id -i ~/.ssh/id_rsa.pub postgres@pghost195\\nssh-copy-id -i ~/.ssh/id_rsa.pub postgres@pghost196\\nssh-copy-id -i ~/.ssh/id_rsa.pub postgres@pghost197\\nssh-copy-id -i ~/.ssh/id_rsa.pub postgres@pghost198\\nssh-copy-id -i ~/.ssh/id_rsa.pub postgres@pghost205\\n \\r\\nДля того чтобы ssh не спрашивала доверяете ли вы хосту и не выдавала другие предупреждения и ограничения, касающиеся политики безопасности, можем добавить в файл Скрытый текст nano /etc/ssh/ssh_config\\n StrictHostKeyChecking no\\n UserKnownHostsFile=/dev/null\\n \\r\\nРестартуем ssh. Данная опция удобная когда вы не слишком заботитесь о безопасности, например для тестирования кластера. \\r\\nПерейдем на ноду 2,3,4 и всё повторим. Теперь мы можем гулять без паролей между нодами для переключения их состояния(назначения нового мастера и standby). Ставим pgbouncer из git \\r\\nУстановим необходимые пакеты для сборки Скрытый текст apt-get install libpq-dev checkinstall build-essential libpam0g-dev libssl-dev libpcre++-dev libtool automake checkinstall gcc+ git -y \\ncd /tmp\\n git clone https://github.com/pgbouncer/pgbouncer.git\\n cd pgbouncer\\ngit submodule init\\ngit submodule update\\n./autogen.sh\\nwget https://github.com/libevent/libevent/releases/download/release-2.0.22-stable/libevent-2.0.22-stable.tar.gz\\ntar -xvf libevent-2.0.22-stable.tar.gz\\ncd libevent*\\n./configure\\ncheckinstall\\ncd ..\\n \\r\\nЕсли хотите postgresql с PAM авторизацией — то ставим еще дом модуль и при configure ставим --with-pam Скрытый текст ./configure --prefix=/usr/local --with-libevent=libevent-prefix --with-pam\\n make -j4\\nmkdir -p /usr/local/share/doc;\\nmkdir -p /usr/local/share/man;\\ncheckinstall\\n \\r\\nСтавим версию — 1.7.2 (на ноябрь 2016 года). \\r\\nГотово. Видим Скрытый текст Done. The new package has been installed and saved to\\n /tmp/pgbouncer/pgbouncer_1.7.2-1_amd64.deb\\n You can remove it from your system anytime using: \\ndpkg -r pgbouncer_1.7.2-1_amd64.deb \\n \\r\\nОбязательно настроим окружение — добавим переменную PATH=/usr/lib/postgresql/9.6/bin:$PATH(на каждой ноде). \\r\\nДобавим в файл ~/.bashrc Скрытый текст su postgres\\ncd ~\\nnano .bashrc\\n \\r\\nВставим код Скрытый текст PATH=$PATH:/usr/lib/postgresql/9.6/bin\\nexport PATH\\nexport PGDATA=\"$HOME/9.6/main\"\\n \\r\\nСохранимся. \\r\\nСкопируем файл на .bashrc другие ноды Скрытый текст su postgres\\ncd ~\\nscp .bashrc postgres@pghost195:/var/lib/postgresql\\nscp .bashrc postgres@pghost196:/var/lib/postgresql\\nscp .bashrc postgres@pghost197:/var/lib/postgresql\\nscp .bashrc postgres@pghost198:/var/lib/postgresql\\nscp .bashrc postgres@pghost205:/var/lib/postgresql\\n Настройке сервера в качестве мастера(pghost195) \\r\\nОтредактируем конфиг /etc/postgresql/9.6/main/postgresql.conf — Приводим к виду необходимые опции(просто добавим в конец файла). Скрытый текст listen_addresses=\\'*\\'\\nwal_level = \\'hot_standby\\'\\narchive_mode = on\\nwal_log_hints = on\\nwal_keep_segments = 0\\narchive_command = \\'cd .\\'\\nmax_replication_slots = 5 # Максимальное количество standby нод, подключенных к мастеру.\\nhot_standby = on\\nshared_preload_libraries = \\'repmgr_funcs, pg_stat_statements\\' ####подключаемая библиотека repmgr и статистики postgres\\nmax_connections = 800\\nmax_wal_senders = 10#максимальное количество одновременных подключений от резервных серверов или клиентов потокового резервного копирования\\nport = 5433\\npg_stat_statements.max = 10000\\npg_stat_statements.track = all\\n \\r\\nКак мы видим — будем запускать postgresql на порту 5433 — потому-что дефолтный порт для приложений будем использовать для других целей — а именно для балансировки, проксирования и failover’a. Вы же можете использовать любой порт, как вам удобно. \\r\\nНастроим файл подключений \\r\\nnano /etc/postgresql/9.6/main/pg_hba.conf \\r\\nПриведем к виду Скрытый текст # IPv6 local connections:\\nhost all all ::1/128 md5\\n\\nlocal all postgres peer\\nlocal all all peer\\nhost all all 127.0.0.1/32 md5\\n\\n#######################################Тут мы настроили соединения для управления репликацией и управления состоянием нод (MASTER, STAND BY).\\nlocal replication repmgr trust\\nhost replication repmgr 127.0.0.1/32 trust\\nhost replication repmgr 10.1.1.0/24 trust\\nlocal repmgr repmgr trust\\nhost repmgr repmgr 127.0.0.1/32 trust\\nhost repmgr repmgr 10.1.1.0/24 trust\\n######################################\\nhost all all 0.0.0.0/32 md5 #######Подключение для всех по паролю\\n#####################################\\n \\r\\nПрименим права к конфигам, иначе будет ругаться на pg_hba.conf Скрытый текст chown -R -v postgres /etc/postgresql\\n \\r\\nСтартуем postgres(от postgres user). Скрытый текст pg_ctl -D /etc/postgresql/9.6/main --log=/var/log/postgresql/postgres_screen.log start\\n \\r\\nНастройка пользователей и базы на Master-сервере(pghost195). Скрытый текст su postgres \\ncd ~\\n \\r\\nСоздадим пользователя repmgr. Скрытый текст psql\\n# create role repmgr with superuser noinherit;\\n# ALTER ROLE repmgr WITH LOGIN;\\n# create database repmgr;\\n# GRANT ALL PRIVILEGES on DATABASE repmgr to repmgr;\\n# ALTER USER repmgr SET search_path TO repmgr_test, \"$user\", public;\\n \\r\\nСоздадим пользователя test_user с паролем 1234 Скрытый текст create user test_user;\\nALTER USER test_user WITH PASSWORD \\'1234\\';\\n \\r\\nКонфигурируем repmgr на master Скрытый текст nano /etc/repmgr.conf\\n \\r\\nСодержимое Скрытый текст cluster=etagi_test\\nnode=1\\nnode_name=node1\\nuse_replication_slots=1\\nconninfo=\\'host=pghost195 port=5433 user=repmgr dbname=repmgr\\'\\npg_bindir=/usr/lib/postgresql/9.6/bin\\n \\r\\nСохраняемся. \\r\\nРегистрируем сервер как мастер. Скрытый текст su postgres\\nrepmgr -f /etc/repmgr.conf master register\\n \\r\\nСмотрим наш статус Скрытый текст repmgr -f /etc/repmgr.conf cluster show\\n \\r\\nВидим Скрытый текст Role | Name | Upstream | Connection String\\n----------+-------|----------|--------------------------------------------------\\n* master | node1 | | host=pghost195 port=5433 user=repmgr dbname=repmgr\\n \\r\\nИдем дальше. Настройка слейвов(standby) — pghost196,pghost197,pghost198 \\r\\nКонфигурируем repmgr на slave1(pghost197) \\r\\nnano /etc/repmgr.conf — создаем конфиг \\r\\nСодержимое Скрытый текст cluster=etagi_test\\nnode=2\\nnode_name=node2\\nuse_replication_slots=1\\nconninfo=\\'host=pghost196 port=5433 user=repmgr dbname=repmgr\\'\\npg_bindir=/usr/lib/postgresql/9.6/bin\\n \\r\\nСохраняемся. \\r\\nРегистрируем сервер как standby Скрытый текст su postgres\\ncd ~/9.6/\\nrm -rf main/*\\nrepmgr -h pghost1 -p 5433 -U repmgr -d repmgr -D main -f /etc/repmgr.conf --copy-external-config-files=pgdata --verbose standby clone\\npg_ctl -D /var/lib/postgresql/9.6/main --log=/var/log/postgresql/postgres_screen.log start\\n \\r\\nБудут скопированы конфиги на основании которых будет происходит переключение состояний master и standby серверов. \\r\\nПросмотрим файлы, которые лежат в корне папки /var/lib/postgresql/9.6/main — обязательно должны быть эти файлы. Скрытый текст PG_VERSION backup_label\\npg_hba.conf pg_ident.conf postgresql.auto.conf postgresql.conf recovery.conf\\n \\r\\nРегистрируем сервер в кластере Скрытый текст su postgres\\nrepmgr -f /etc/repmgr.conf standby register; repmgr -f /etc/repmgr.conf cluster show\\nПросмотр состояния кластера\\nrepmgr -f /etc/repmgr.conf cluster show\\nВидим\\n\\n&lt;spoiler title=\"\"&gt;\\n&lt;source lang=\"bash\"&gt;\\nRole | Name | Upstream | Connection String\\n----------+-------|----------|--------------------------------------------------\\n* master | node195 | | host=pghost195 port=5433 user=repmgr dbname=repmgr\\n standby | node196 | node1 | host=pghost196 port=5433 user=repmgr dbname=repmgr\\n Настройка второго stand-by — pghost197 Конфигурируем repmgr на pghost197 \\r\\nnano /etc/repmgr.conf — создаем конфиг \\r\\nСодержимое Скрытый текст cluster=etagi_test\\nnode=3\\nnode_name=node3\\nuse_replication_slots=1\\nconninfo=\\'host=pghost197 port=5433 user=repmgr dbname=repmgr\\'\\npg_bindir=/usr/lib/postgresql/9.6/bin\\n \\r\\nСохраняемся. \\r\\nРегистрируем сервер как standby Скрытый текст su postgres\\ncd ~/9.6/\\nrm -rf main/*\\nrepmgr -h pghost195 -p 5433 -U repmgr -d repmgr -D main -f /etc/repmgr.conf --copy-external-config-files=pgdata --verbose standby clone\\n \\r\\nили Скрытый текст repmgr -D /var/lib/postgresql/9.6/main -f /etc/repmgr.conf -d repmgr -p 5433 -U repmgr -R postgres --verbose --force --rsync-only --copy-external-config-files=pgdata standby clone -h pghost195\\n \\r\\nДанная команда с опцией -r/--rsync-only — используется в некоторых случаях, например, когда копируемый каталог данных — это каталог данных отказавшего сервера с активным узлом репликации. \\r\\nТакже будут скопированы конфиги на основании которых будет происходит переключение состояний master и standby серверов. \\r\\nПросмотрим файлы, которые лежат в корне папки /var/lib/postgresql/9.6/main — обязательно должны быть следующие файлы: Скрытый текст PG_VERSION backup_label\\npg_hba.conf pg_ident.conf postgresql.auto.conf postgresql.conf recovery.conf\\n \\r\\nСтартуем postgres(от postgres) Скрытый текст pg_ctl -D /var/lib/postgresql/9.6/main --log=/var/log/postgresql/postgres_screen.log start\\n \\r\\nРегистрируем сервер в кластере Скрытый текст su postgres\\nrepmgr -f /etc/repmgr.conf standby register\\n \\r\\nПросмотр состояния кластера Скрытый текст repmgr -f /etc/repmgr.conf cluster show\\n \\r\\nВидим Скрытый текст Role | Name | Upstream | Connection String\\n----------+-------|----------|--------------------------------------------------\\n* master | node1 | | host=pghost1 port=5433 user=repmgr dbname=repmgr\\n standby | node2 | node1 | host=pghost2 port=5433 user=repmgr dbname=repmgr\\n standby | node3 | node1 | host=pghost2 port=5433 user=repmgr dbname=re\\n Настройка каскадной репликации. \\r\\nВы также можете настроить каскадную репликацию. Рассмотрим пример. Конфигурируем repmgr на pghost198 от pghost197 \\r\\nnano /etc/repmgr.conf — создаем конфиг. Содержимое Скрытый текст cluster=etagi_test\\nnode=4\\nnode_name=node4\\nuse_replication_slots=1\\nconninfo=\\'host=pghost198 port=5433 user=repmgr dbname=repmgr\\'\\npg_bindir=/usr/lib/postgresql/9.6/bin\\nupstream_node=3\\n \\r\\nСохраняемся. Как мы видим, что в upstream_node мы указали node3, которой является pghost197. \\r\\nРегистрируем сервер как standby от standby Скрытый текст su postgres\\ncd ~/9.6/\\nrm -rf main/*\\nrepmgr -h pghost197 -p 5433 -U repmgr -d repmgr -D main -f /etc/repmgr.conf --copy-external-config-files=pgdata --verbose standby clone\\n \\r\\nСтартуем postgres(от postgres) Скрытый текст pg_ctl -D /var/lib/postgresql/9.6/main --log=/var/log/postgresql/postgres_screen.log start\\n \\r\\nРегистрируем сервер в кластере Скрытый текст su postgres\\nrepmgr -f /etc/repmgr.conf standby register\\n \\r\\nПросмотр состояния кластера Скрытый текст repmgr -f /etc/repmgr.conf cluster show\\n \\r\\nВидим Скрытый текст Role | Name | Upstream | Connection String\\n----------+-------|----------|--------------------------------------------------\\n* master | node1 | | host=pghost195 port=5433 user=repmgr dbname=repmgr\\n standby | node2 | node1 | host=pghost196 port=5433 user=repmgr dbname=repmgr\\n standby | node3 | node1 | host=pghost197 port=5433 user=repmgr dbname=repmgr\\n standby | node4 | node3 | host=pghost198 port=5433 user=repmgr dbname=repmgr\\n Настройка Автоматического Failover\\'а. \\r\\nИтак мы закончили настройку потоковой репликации. Теперь перейдем к настройка автопереключения — активации нового мастера из stand-by сервера. Для этого необходимо добавить новые секции в файл /etc/repmgr.conf на stand-by серверах. На мастере этого быть не должно! \\r\\n! Конфиги на standby (slave’s) должны отличаться — как в примере ниже. Выставим разное время(master_responce_timeout)! \\r\\nДобавляем строки на pghost196 в /etc/repmgr.conf Скрытый текст #######АВТОМАТИЧЕСКИЙ FAILOVER#######ТОЛЬКО НА STAND BY##################\\nmaster_response_timeout=20\\nreconnect_attempts=5\\nreconnect_interval=5\\nfailover=automatic\\npromote_command=\\'sh /etc/postgresql/failover_promote.sh\\'\\nfollow_command=\\'sh /etc/postgresql/failover_follow.sh\\'\\n#loglevel=NOTICE\\n#logfacility=STDERR\\n#logfile=\\'/var/log/postgresql/repmgr-9.6.log\\'\\npriority=90 # a value of zero or less prevents the node being promoted to master\\n \\r\\nДобавляем строки на pghost197 в /etc/repmgr.conf Скрытый текст #######АВТОМАТИЧЕСКИЙ FAILOVER#######ТОЛЬКО НА STAND BY##################\\nmaster_response_timeout=20\\nreconnect_attempts=5\\nreconnect_interval=5\\nfailover=automatic\\npromote_command=\\'sh /etc/postgresql/failover_promote.sh\\'\\nfollow_command=\\'sh /etc/postgresql/failover_follow.sh\\'\\n#loglevel=NOTICE\\n#logfacility=STDERR\\n#logfile=\\'/var/log/postgresql/repmgr-9.6.log\\'\\npriority=70 # a value of zero or less prevents the node being promoted to master\\n \\r\\nДобавляем строки на pghost198 в /etc/repmgr.conf Скрытый текст #######АВТОМАТИЧЕСКИЙ FAILOVER#######ТОЛЬКО НА STAND BY##################\\nmaster_response_timeout=20\\nreconnect_attempts=5\\nreconnect_interval=5\\nfailover=automatic\\npromote_command=\\'sh /etc/postgresql/failover_promote.sh\\'\\nfollow_command=\\'sh /etc/postgresql/failover_follow.sh\\'\\n#loglevel=NOTICE\\n#logfacility=STDERR\\n#logfile=\\'/var/log/postgresql/repmgr-9.6.log\\'\\npriority=50 # a value of zero or less prevents the node being promoted to master\\n \\r\\nКак мы видим все настройки автофейоловера идентичны, разница только в priority. Если 0, то данный Standby никогда не станет Master. Данный параметр будет определять очередность срабатывания failover’a, т.е. меньшее число говорит о большем приоритете, значит после отказа master сервера его функции на себя возьмет pghost197. \\r\\nТакже необходимо добавить следующие строки в файл /etc/postgresql/9.6/main/postgresql.conf (только на stand-by сервера!!!!!!) Скрытый текст shared_preload_libraries = \\'repmgr_funcs\\'\\n \\r\\nДля запуска демона детектирования автоматического переключения необходимо: Скрытый текст su postgres\\nrepmgrd -f /etc/repmgr.conf -p /var/run/postgresql/repmgrd.pid -m -d -v &gt;&gt; /var/log/postgresql/repmgr.log 2&gt;&amp;1\\n \\r\\nПроцесс repmgrd будет запущен как демон. Смотрим Скрытый текст ps aux | grep repmgrd\\n \\r\\nВидим Скрытый текст postgres 2921 0.0 0.0 59760 5000 ? S 16:54 0:00 /usr/lib/postgresql/9.6/bin/repmgrd -f /etc/repmgr.conf -p /var/run/postgresql/repmgrd.pid -m -d -v\\npostgres 3059 0.0 0.0 12752 2044 pts/1 S+ 16:54 0:00 grep repmgrd\\n \\r\\nВсё ок. Идём дальше. \\r\\nПроверим работу автофейловера Скрытый текст su postgres\\npsql repmgr\\nrepmgr # SELECT * FROM repmgr_etagi_test.repl_nodes ORDER BY id;\\n\\n\\n id | type | upstream_node_id | cluster | name | conninfo | slot_name | priority | active \\n----+---------+------------------+------------+-------+---------------------------------------------------+---------------+----------+--------\\n 1 | master | | etagi_test | node1 | host=pghost195 port=5433 user=repmgr dbname=repmgr | repmgr_slot_1 | 100 | t\\n 2 | standby | 1 | etagi_test | node2 | host=pghost196 port=5433 user=repmgr dbname=repmgr | repmgr_slot_2 | 100 | t\\n 3 | standby | 1 | etagi_test | node3 | host=pghost197 port=5433 user=repmgr dbname=repmgr | repmgr_slot_3 | 100 | t\\n \\r\\nПока все нормально — теперь проведем тест. Остановим мастер — pghost195 Скрытый текст su postgres\\npg_ctl -D /etc/postgresql/9.6/main -m immediate stop\\n \\r\\nВ логах на pghost196 Скрытый текст tail -f /var/log/postgresql/*\\n \\r\\nВидим Скрытый текст [2016-10-21 16:58:34] [NOTICE] promoting standby\\n[2016-10-21 16:58:34] [NOTICE] promoting server using \\'/usr/lib/postgresql/9.6/bin/pg_ctl -D /var/lib/postgresql/9.6/main promote\\'\\n[2016-10-21 16:58:36] [NOTICE] STANDBY PROMOTE successful\\n \\r\\nВ логах на pghost197 Скрытый текст tail -f /var/log/postgresql/*\\n \\r\\nВидим Скрытый текст 2016-10-21 16:58:39] [NOTICE] node 2 is the best candidate for new master, attempting to follow...\\n[2016-10-21 16:58:40] [ERROR] connection to database failed: could not connect to server: Connection refused\\n\\tIs the server running on host \"pghost195\" (10.1.1.195) and accepting\\n\\tTCP/IP connections on port 5433?\\n\\n[2016-10-21 16:58:40] [NOTICE] restarting server using \\'/usr/lib/postgresql/9.6/bin/pg_ctl -w -D /var/lib/postgresql/9.6/main -m fast restart\\'\\n[2016-10-21 16:58:42] [NOTICE] node 3 now following new upstream node 2\\n \\r\\nВсё работает. У нас новый мастер — pghost196, pghost197,pghost198 — теперь слушает stream от pghost2. Возвращение упавшего мастера в строй! \\r\\nНельзя просто так взять и вернуть упавший мастер в строй. Но он вернется в качестве слейва. \\r\\nPostgres должна быть остановлена перед процедурой возвращения. На ноде, которая отказала создаем скрипт. В этом скрипт уже настроено уведомление телеграмм, и настроена проверка по триггеру — если создан файл /etc/postgresql/disabled, то восстановление не произойдет. Так же создадим файл /etc/postgresql/current_master.list с содержимым — именем текущего master. /etc/postgresql/current_master.list pghost196\\n \\r\\nНазовем скрипт «register.sh» и разместим в каталоге /etc/postgresql \\r\\nСкрипт восстановления ноды в кластер в качестве standby /etc/postgresql/register.sh. \\ntrigger=\"/etc/postgresql/disabled\"\\nTEXT=\"\\'`hostname -f`_postgresql_disabled_and_don\\'t_be_started.You_must_delete_file_/etc/postgresql/disabled\\'\"\\nTEXT\\nif [ -f \"$trigger\" ]\\nthen\\n\\techo \"Current server is disabled\"\\n\\tsh /etc/postgresql/telegram.sh $TEXT\\nelse\\n\\npkill repmgrd\\npg_ctl stop\\nrm -rf /var/lib/postgresql/9.6/main/*;\\nmkdir /var/run/postgresql/9.6-main.pg_stat_tmp;\\n#repmgr -D /var/lib/postgresql/9.6/main -f /etc/repmgr.conf -d repmgr -p 5433 -U repmgr -R postgres --verbose --force --rsync-only --copy-external-config-files=pgdata standby clone -h $(cat /etc/postgresql/current_master.list);\\nrepmgr -h $(cat /etc/postgresql/current_master.list) -p 5433 -U repmgr -d repmgr -D /var/lib/postgresql/9.6/main -f /etc/repmgr.conf --copy-external-config-files=pgdata --verbose standby clone\\n/usr/lib/postgresql/9.6/bin/pg_ctl -D /var/lib/postgresql/9.6/main --log=/var/log/postgresql/postgres_screen.log start;\\n/bin/sleep 5;\\nrepmgr -f /etc/repmgr.conf --force standby register;\\necho \"Вывод состояния кластера\";\\nrepmgr -f /etc/repmgr.conf cluster show;\\nsh /etc/postgresql/telegram.sh $TEXT\\nsh /etc/postgresql/repmgrd.sh;\\nps aux | grep repmgrd;\\nfi\\n \\r\\nКак вы видите у нас также есть в скрипте файл repmgrd.sh и telegram.sh. Они также должны находится в каталоге /etc/postgresql. /etc/postgresql/repmgrd.sh #!/bin/bash\\npkill repmgrd\\nrm /var/run/postgresql/repmgrd.pid;\\nrepmgrd -f /etc/repmgr.conf -p /var/run/postgresql/repmgrd.pid -m -d -v &gt;&gt; /var/log/postgresql/repmgr.log 2&gt;&amp;1;\\nps aux | grep repmgrd;\\n \\r\\nСкрипт для отправки в телеграмм. telegram.sh. \\nUSERID=\"Юзер_ид_пользователей_телеграм_через_пробел\"\\nCLUSTERNAME=\"PGCLUSTER_RIES\"\\nKEY=\"Ключ_бота_телеграм\"\\nTIMEOUT=\"10\"\\nEXEPT_USER=\"root\"\\nURL=\"https://api.telegram.org/bot$KEY/sendMessage\"\\nDATE_EXEC=\"$(date \"+%d %b %Y %H:%M\")\"\\nTMPFILE=\\'/etc/postgresql/ipinfo-$DATE_EXEC.txt\\'\\n IP=$(echo $SSH_CLIENT | awk \\'{print $1}\\')\\n PORT=$(echo $SSH_CLIENT | awk \\'{print $3}\\')\\n HOSTNAME=$(hostname -f)\\n IPADDR=$(hostname -I | awk \\'{print $1}\\')\\n curl http://ipinfo.io/$IP -s -o $TMPFILE\\n #ORG=$(cat $TMPFILE | jq \\'.org\\' | sed \\'s/\"//g\\')\\n TEXT=$1\\n for IDTELEGRAM in $USERID\\n do\\n curl -s --max-time $TIMEOUT -d \"chat_id=$IDTELEGRAM&amp;disable_web_page_preview=1&amp;text=$TEXT\" $URL &gt; /dev/null\\n done\\n rm $TMPFILE\\n\\n \\r\\nОтредактируем конфиг repmgr на упавшем мастере /etc/repmgr.conf cluster=etagi_cluster1\\nnode=1\\nnode_name=node195\\nuse_replication_slots=1\\nconninfo=\\'host=pghost195 port=5433 user=repmgr dbname=repmgr\\'\\npg_bindir=/usr/lib/postgresql/9.6/bin\\n#######АВТОМАТИЧЕСКИЙ FAILOVER#######ТОЛЬКО НА STAND BY##################\\n\\nmaster_response_timeout=20\\nreconnect_attempts=5\\nreconnect_interval=5\\nfailover=automatic\\npromote_command=\\'sh /etc/postgresql/failover_promote.sh\\'\\nfollow_command=\\'sh /etc/postgresql/failover_follow.sh\\'\\n#loglevel=NOTICE\\n#logfacility=STDERR\\n#logfile=\\'/var/log/postgresql/repmgr-9.6.log\\'\\npriority=95 # a value of zero or less prevents the node being promoted to master\\n \\r\\nСохранимся. Теперь запустим наш скрипт, на отказавшей ноде. Не забываем про права(postgres) для файлов. \\r\\nsh /etc/postgresq/register.sh \\r\\nУвидим Скрытый текст [2016-10-31 15:19:53] [NOTICE] notifying master about backup completion...\\nЗАМЕЧАНИЕ: команда pg_stop_backup завершена, все требуемые сегменты WAL заархивированы\\n[2016-10-31 15:19:54] [NOTICE] standby clone (using rsync) complete\\n[2016-10-31 15:19:54] [NOTICE] you can now start your PostgreSQL server\\n[2016-10-31 15:19:54] [HINT] for example : pg_ctl -D /var/lib/postgresql/9.6/main start\\n[2016-10-31 15:19:54] [HINT] After starting the server, you need to register this standby with \"repmgr standby register\"\\nсервер запускается\\n[2016-10-31 15:19:59] [NOTICE] standby node correctly registered for cluster etagi_cluster1 with id 2 (conninfo: host=pghost196 port=5433 user=repmgr dbname=repmgr)\\nВывод состояния кластера\\nRole | Name | Upstream | Connection String\\n----------+---------|----------|----------------------------------------------------\\n* standby | node195 | | host=pghost195 port=5433 user=repmgr dbname=repmgr\\n master | node196 | node195 | host=pghost197 port=5433 user=repmgr dbname=repmgr\\n standby | node197 | node195 | host=pghost198 port=5433 user=repmgr dbname=repmgr\\n standby | node198 | node195 | host=pghost196 port=5433 user=repmgr dbname=repmgr\\npostgres 11317 0.0 0.0 4336 716 pts/0 S+ 15:19 0:00 sh /etc/postgresql/repmgrd.sh\\npostgres 11322 0.0 0.0 59548 3632 ? R 15:19 0:00 /usr/lib/postgresql/9.6/bin/repmgrd -f /etc/repmgr.conf -p /var/run/postgresql/repmgrd.pid -m -d -v\\npostgres 11324 0.0 0.0 12752 2140 pts/0 S+ 15:19 0:00 grep repmgrd\\npostgres 11322 0.0 0.0 59548 4860 ? S 15:19 0:00 /usr/lib/postgresql/9.6/bin/repmgrd -f /etc/repmgr.conf -p /var/run/postgresql/repmgrd.pid -m -d -v\\npostgres 11327 0.0 0.0 12752 2084 pts/0 S+ 15:19 0:00 grep repmgrd\\n \\r\\nКак мы видим скрипт отработал, мы получили уведомления и увидели состояние кластера. Реализации процедуры Switchover(смены мастера вручную). \\r\\nДопустим наступила такая ситуация, когда вам необходимо поменять местами мастер и определенный standby. Допустим хотим сделать мастером pghost195 вместо ставшего по фейловеру pghost196, после его восстановления в качестве слейва. Наши шаги. \\r\\nНа pghost195 Скрытый текст su postgres\\nrepmgr -f /etc/repmgr.conf standby switchover \\nВидим\\n[2016-10-26 15:29:42] [NOTICE] replication slot \"repmgr_slot_1\" deleted on former master\\n[2016-10-26 15:29:42] [NOTICE] switchover was successful\\n \\r\\nТеперь нам необходимо дать команду репликам, кроме старого мастера, дать команду на перенос на новый мастер \\r\\nНа pghost197 Скрытый текст su postgres\\nrepmgr -f /etc/repmgr.conf standby follow\\nrepmgr -f /etc/repmgr.conf cluster show;\\n \\r\\nВидим что мы следуем за новым мастером. \\r\\nНа pghost198 — то же самое Скрытый текст su postgres\\nrepmgr -f /etc/repmgr.conf standby follow\\nrepmgr -f /etc/repmgr.conf cluster show;\\n \\r\\nВидим что мы следуем за новым мастером. \\r\\nНа pghost196 — он был предыдущим мастером, у которого мы отобрали права Скрытый текст su postgres\\nrepmgr -f /etc/repmgr.conf standby follow\\n \\r\\nВидим ошибку Скрытый текст [2016-10-26 15:35:51] [ERROR] Slot \\'repmgr_slot_2\\' already exists as an active slot\\n \\r\\nCтопаем pghost196 Скрытый текст pg_ctl stop\\n \\r\\nДля ее исправления идем на phgost195(новый мастер) Скрытый текст su postgres\\npsql repmgr\\n#select pg_drop_replication_slot(\\'repmgr_slot_2\\');\\n \\r\\nВидим Скрытый текст pg_drop_replication_slot \\n--------------------------\\n(1 row)\\n \\r\\nИдем на pghost196, и делаем все по аналогии с пунктом. Создание и использование witness ноды \\r\\nWitness нода используется для управления кластером, в случае наступления файловера и выступает своего рода арбитром, следит за тем чтобы не наступали конфликтные ситуации при выборе нового мастера. Она не является активной нодой в плане использования как standby сервера, может быть установлена на той же ноде что и postgres или на отдельной ноде. \\r\\nДобавим еще одну ноду pghost205 для управления кластером( настройка абсолютно аналогична настройке слейва), толь будет отличаться способ копирования: Скрытый текст repmgr -h pghost195 -p 5433 -U repmgr -d repmgr -D main -f /etc/repmgr.conf --force --copy-external-config-files=pgdata --verbose witness create;\\nили\\nrepmgr -D /var/lib/postgresql/9.6/main -f /etc/repmgr.conf -d repmgr -p 5433 -U repmgr -R postgres --verbose --force --rsync-only --copy-external-config-files=pgdata witness create -h pghost195;\\n \\r\\nУвидим вывод Скрытый текст 2016-10-26 17:27:06] [WARNING] --copy-external-config-files can only be used when executing STANDBY CLONE\\n[2016-10-26 17:27:06] [NOTICE] using configuration file \"/etc/repmgr.conf\"\\nФайлы, относящиеся к этой СУБД, будут принадлежать пользователю \"postgres\".\\nОт его имени также будет запускаться процесс сервера.\\nКластер баз данных будет инициализирован с локалью \"ru_RU.UTF-8\".\\nКодировка БД по умолчанию, выбранная в соответствии с настройками: \"UTF8\".\\nВыбрана конфигурация текстового поиска по умолчанию \"russian\".\\n\\n\\nКонтроль целостности страниц данных отключен.\\n\\n\\nисправление прав для существующего каталога main... ок\\nсоздание подкаталогов... ок\\nвыбирается значение max_connections... 100\\nвыбирается значение shared_buffers... 128MB\\nвыбор реализации динамической разделяемой памяти ... posix\\nсоздание конфигурационных файлов... ок\\nвыполняется подготовительный скрипт ... ок\\nвыполняется заключительная инициализация ... ок\\nсохранение данных на диске... ок\\n\\n\\nПРЕДУПРЕЖДЕНИЕ: используется проверка подлинности \"trust\" для локальных подключений.\\nДругой метод можно выбрать, отредактировав pg_hba.conf или используя ключи -A,\\n--auth-local или --auth-host при следующем выполнении initdb.\\n\\n\\nГотово. Теперь вы можете запустить сервер баз данных:\\n\\n\\n /usr/lib/postgresql/9.6/bin/pg_ctl -D main -l logfile start\\n\\n\\nожидание запуска сервера....СООБЩЕНИЕ: система БД была выключена: 2016-10-26 17:27:07 YEKT\\nСООБЩЕНИЕ: Защита от наложения мультитранзакций сейчас включена\\nСООБЩЕНИЕ: система БД готова принимать подключения\\nСООБЩЕНИЕ: процесс запуска автоочистки создан\\n готово\\nсервер запущен\\nWarning: Permanently added \\'pghost1,10.1.9.1\\' (ECDSA) to the list of known hosts.\\nreceiving incremental file list\\npg_hba.conf\\n 1,174 100% 1.12MB/s 0:00:00 (xfr#1, to-chk=0/1)\\nСООБЩЕНИЕ: получен SIGHUP, файлы конфигурации перезагружаются\\nсигнал отправлен серверу\\n[2016-10-26 17:27:10] [NOTICE] configuration has been successfully copied to the witness\\n\\n/usr/lib/postgresql/9.6/bin/pg_ctl -D /var/lib/postgresql/9.6/main -l logfile start\\n \\r\\nГотово. Идем далее. Правим файл repmgr.conf для witness ноды \\r\\nОтключаем автоматический файловер на ноде witness nano /etc/repmgr.conf cluster=etagi_test\\nnode=5\\nnode_name=node5\\nuse_replication_slots=1\\nconninfo=\\'host=pghost205 port=5499 user=repmgr dbname=repmgr\\'\\npg_bindir=/usr/lib/postgresql/9.6/bin\\n\\n#######FAILOVER#######ТОЛЬКО НА WITNESS NODE#######\\nmaster_response_timeout=50\\nreconnect_attempts=3\\nreconnect_interval=5\\nfailover=manual\\npromote_command=\\'repmgr standby promote -f /etc/repmgr.conf\\'\\nfollow_command=\\'repmgr standby follow -f /etc/repmgr.conf\\'\\n \\r\\nНа witness ноде обязательно изменить порт на 5499 в conninfo. \\r\\nОбязательно (пере)запускаем repmgrd на всех нодах, кроме мастера Скрытый текст su postgres\\npkill repmgr\\nrepmgrd -f /etc/repmgr.conf -p /var/run/postgresql/repmgrd.pid -m -d -v &gt;&gt; /var/log/postgresql/repmgr.log 2&gt;&amp;1\\nps aux | grep repmgr\\n Настройка менеджера соединений Pgbouncer и балансировки через Haproxy. Отказоустойчивости через Keepalived. Настройка Pgbouncer \\r\\nPgbouncer мы уже установили заранее. Для чего он нужен… Зачем Pgbouncer Мультиплексором соединений. Он выглядит как обычный процесс Postgres, но внутри он управляет очередями запросов что позволяет в разы ускорить работу сервера. Из тысяч запросов поступивших к PgBouncer до базы данных дойдет всего несколько десятков. \\n \\r\\nПерейдем к его настройке. \\r\\nСкопируем установленный pgbouncer в папку /etc/(для удобства) Скрытый текст cp -r /usr/local/share/doc/pgbouncer /etc\\ncd /etc/pgbouncer\\n \\r\\nПриведем к виду файл в nano /etc/pgbouncer/pgbouncer.ini [databases]\\n################################ПОДКЛ К БАЗЕ###########\\nweb1 = host = localhost port=5433 dbname=web1\\nweb2 = host = localhost port=5433 dbname=web2\\n#######################################################\\n[pgbouncer]\\nlogfile = /var/log/postgresql/pgbouncer.log\\npidfile = /var/run/postgresql/pgbouncer.pid\\nlisten_addr = *\\nlisten_port = 6432\\nauth_type = trust\\nauth_file = /etc/pgbouncer/userlist.txt\\n\\n;;; Pooler personality questions\\n\\n; When server connection is released back to pool:\\n; session - after client disconnects\\n; transaction - after transaction finishes\\n; statement - after statement finishes\\npool_mode = session\\nserver_reset_query = DISCARD ALL\\nmax_client_conn = 500\\ndefault_pool_size = 30\\n \\r\\nОтредактируем файл /etc/pgbouncer/userlist.txt \"test_user\" \"passworduser\"\\n\"postgres\" \"passwordpostgres\"\\n\"pgbouncer\" \"fake\"\\n \\r\\nПрименим права Скрытый текст chown -R postgres /etc/pgbouncer\\n \\r\\nПосле редактирования запустим командой как демон (-d) Запуск pgbouncer su postgres\\npkill pgbouncer\\npgbouncer -d --verbose /etc/pgbouncer/pgbouncer.ini \\n \\r\\nСмотрим порт Скрытый текст netstat -4ln | grep 6432\\n \\r\\nСмотрим лог Скрытый текст tail -f /var/log/postgresql/pgbouncer.log\\n \\r\\nПробуем подключиться. Повторяем все тоже на всех нодах. Установка и настройка Haproxy. \\r\\nСтавим Xinetd и Haproxy Скрытый текст apt-get install xinetd haproxy -y\\n \\r\\nДобавляем строку в конец файла Скрытый текст nano /etc/services\\npgsqlchk 23267/tcp # pgsqlchk\\n \\r\\nУстанавливаем скрипт для проверки состояния postgres — pgsqlcheck nano /opt/pgsqlchk #!/bin/bash\\n# /opt/pgsqlchk \\n# This script checks if a postgres server is healthy running on localhost. It will\\n# return:\\n#\\n# \"HTTP/1.x 200 OK\\\\r\" (if postgres is running smoothly)\\n#\\n# - OR -\\n#\\n# \"HTTP/1.x 500 Internal Server Error\\\\r\" (else)\\n#\\n# The purpose of this script is make haproxy capable of monitoring postgres properly\\n#\\n#\\n# It is recommended that a low-privileged postgres user is created to be used by\\n# this script.\\n# For eg. create user pgsqlchkusr login password \\'pg321\\';\\n#\\n \\nPGSQL_HOST=\"localhost\"\\nPGSQL_PORT=\"5433\"\\nPGSQL_DATABASE=\"template1\"\\nPGSQL_USERNAME=\"pgsqlchkusr\"\\nexport PGPASSWORD=\"pg321\"\\n \\nTMP_FILE=\"/tmp/pgsqlchk.out\"\\nERR_FILE=\"/tmp/pgsqlchk.err\"\\n \\n \\n#\\n# We perform a simple query that should return a few results :-p\\n#\\npsql -h $PGSQL_HOST -p $PGSQL_PORT -U $PGSQL_USERNAME \\\\\\n $PGSQL_DATABASE -c \"show port;\" &gt; $TMP_FILE 2&gt; $ERR_FILE\\n \\n#\\n# Check the output. If it is not empty then everything is fine and we return\\n# something. Else, we just do not return anything.\\n#\\nif [ \"$(/bin/cat $TMP_FILE)\" != \"\" ]\\nthen\\n # Postgres is fine, return http 200\\n /bin/echo -e \"HTTP/1.1 200 OK\\\\r\\\\n\"\\n /bin/echo -e \"Content-Type: Content-Type: text/plain\\\\r\\\\n\"\\n /bin/echo -e \"\\\\r\\\\n\"\\n /bin/echo -e \"Postgres is running.\\\\r\\\\n\"\\n /bin/echo -e \"\\\\r\\\\n\"\\nelse\\n # Postgres is down, return http 503\\n /bin/echo -e \"HTTP/1.1 503 Service Unavailable\\\\r\\\\n\"\\n /bin/echo -e \"Content-Type: Content-Type: text/plain\\\\r\\\\n\"\\n /bin/echo -e \"\\\\r\\\\n\"\\n /bin/echo -e \"Postgres is *down*.\\\\r\\\\n\"\\n /bin/echo -e \"\\\\r\\\\n\"\\nfi\\n \\r\\nСоответственно нам необходимо добавить пользователя pgsqlchkusr для проверки состояния postgres Скрытый текст plsq\\n#create user pgsqlchkusr;\\n#ALTER ROLE pgsqlchkusr WITH LOGIN;\\n#ALTER USER pgsqlchkusr WITH PASSWORD \\'pg321\\';\\n#\\\\q\\n \\r\\nДелаем скрипт исполняемым и даем права временных файлам — иначе check не сработает. Скрытый текст chmod +x /opt/pgsqlchk;touch /tmp/pgsqlchk.out; touch /tmp/pgsqlchk.err; chmod 777 /tmp/pgsqlchk.out; chmod 777 /tmp/pgsqlchk.err;\\n \\r\\nСоздаем конфиг файл xinetd для pgsqlchk nano /etc/xinetd.d/pgsqlchk \\n# /etc/xinetd.d/pgsqlchk\\n# # default: on\\n# # description: pqsqlchk\\nservice pgsqlchk\\n{\\n flags = REUSE\\n socket_type = stream\\n port = 23267\\n wait = no\\n user = nobody\\n server = /opt/pgsqlchk\\n log_on_failure += USERID\\n disable = no\\n only_from = 0.0.0.0/0\\n per_source = UNLIMITED\\n\\n}\\n \\r\\nСохраняемся. Настраиваем haproxy. \\r\\nРедактируем конфиг — удалим старый и вставим это содержимое. Этот конфиг для первой ноды, на которой крутится мастер, на данный момент допустим, что это pghost195. Соответственно для данного хоста мы сделаем активным в пуле соединений свой-же хост, работающий на порте 6432(через pgbouncer). nano /etc/haproxy/haproxy.cfg \\nglobal\\nlog 127.0.0.1 local0\\nlog 127.0.0.1 local1 notice\\n#chroot /usr/share/haproxy\\nchroot /var/lib/haproxy\\npidfile /var/run/haproxy.pid\\n\\nuser postgres\\ngroup postgres\\ndaemon\\nmaxconn 20000\\ndefaults\\nlog global\\nmode http\\noption tcplog\\noption dontlognull\\nretries 3\\noption redispatch\\ntimeout connect 30000ms\\ntimeout client 30000ms\\ntimeout server 30000ms\\n\\nfrontend stats-front\\nbind *:8080\\nmode http\\ndefault_backend stats-back\\n\\nfrontend pxc-onenode-front\\nbind *:5432\\nmode tcp\\ndefault_backend pxc-onenode-back\\n\\nbackend stats-back\\nmode http\\nstats uri /\\nstats auth admin:adminpassword\\n\\nbackend pxc-onenode-back\\n mode tcp\\n balance leastconn\\n option httpchk\\n default-server port 6432 inter 2s downinter 5s rise 3 fall 2 slowstart 60s maxqueue 128 weight 100\\n server pghost195 10.1.1.195:6432 check port 23267 \\n \\r\\nСам порт haproxy для подключения к базе крутится на порте 5432. Админка доступна на порте 8080. Пользователь admin с паролем adminpassword. \\r\\nРестартим сервисы Скрытый текст /etc/init.d/xinetd restart;\\n/etc/init.d/haproxy restart;\\n \\r\\nТоже самое делаем еще на всех нодах. На той ноде, которую вы хотите сделать балансировщиком, например pghost198(запросы на нее будут идти только на чтение) конфиг haproxy приводим к такому виду. nano /etc/haproxy/haproxy.cfg global\\nlog 127.0.0.1 local0\\nlog 127.0.0.1 local1 notice\\n#chroot /usr/share/haproxy\\nchroot /var/lib/haproxy\\npidfile /var/run/haproxy.pid\\nuser postgres\\ngroup postgres\\ndaemon\\nmaxconn 20000\\ndefaults\\nlog global\\nmode http\\noption tcplog\\noption dontlognull\\nretries 3\\noption redispatch\\ntimeout connect 30000ms\\ntimeout client 30000ms\\ntimeout server 30000ms\\n\\nfrontend stats-front\\nbind *:8080\\nmode http\\ndefault_backend stats-back\\n\\nfrontend pxc-onenode-front\\nbind *:5432\\nmode tcp\\ndefault_backend pxc-onenode-back\\n\\nbackend stats-back\\nmode http\\nstats uri /\\nstats auth admin:adminpassword\\n\\nbackend pxc-onenode-back\\n mode tcp\\n balance roundrobin\\n option httpchk\\n default-server port 6432 inter 2s downinter 5s rise 3 fall 2 slowstart 60s maxqueue 128 weight 100\\n server pghost196 10.1.1.196:6432 check port 23267\\n server pghost197 10.1.1.196:6432 check port 23267\\n server pghost198 10.1.1.196:6432 check port 23267\\n \\r\\nСтатистику смотри на hostip :8080 Установка keepalived. \\r\\nKeepalived позволяет использовать виртуальный ip адрес (VIP) и в случае выходы из строя одной из нод(выключение питания или другое событие) ip адрес перейдет на другую ноду. Например у нас будет VIP 10.1.1.192 между нодой pghost195,pghost196,pghost197. Соответвенно при выключение питании на ноде pghost195 нода pghost196 автоматически присвоит себе ip addr 10.1.1.192 и так как она является второй в приоритете на продвижение к роли мастера станет доступной для записи благодаря или haproxy или pgbouncer — тут все зависит от вашего выбора. В нашем сценарии — это Haproxy. \\r\\nСтавим keepalived Скрытый текст apt-get install keepalived -y\\n \\r\\nНастраиваем keepalived. На всех нодах в /etc/sysctl.conf добавим Скрытый текст net.ipv4.ip_forward=1\\n \\r\\nЗатем Скрытый текст sysctl -p\\n \\r\\nНА 1-ой ноде(pghost195) nano /etc/keepalived/keepalived.conf ! this is who emails will go to on alerts\\n notification_email {\\n admin@domain.com\\n\\n ! add a few more email addresses here if you would like\\n }\\n notification_email_from servers@domain.com\\n ! I use the local machine to relay mail\\n smtp_server smt.local.domain\\n smtp_connect_timeout 30\\n ! each load balancer should have a different ID\\n ! this will be used in SMTP alerts, so you should make\\n ! each router easily identifiable\\n lvs_id LVS_HAPROXY-pghost195\\n}\\n\\n}\\nvrrp_instance haproxy-pghost195 {\\n interface eth0\\n state MASTER\\n virtual_router_id 192\\n priority 150\\n ! send an alert when this instance changes state from MASTER to BACKUP\\n smtp_alert\\n authentication {\\n auth_type PASS\\n auth_pass passwordforcluster\\n }\\n track_script {\\n chk_http_port\\n }\\n virtual_ipaddress {\\n 10.1.1.192/32 dev eth0\\n }\\n notify_master \"sh /etc/postgresql/telegram.sh \\'MASTER pghost195.etagi.com получил VIP\\'\" \\n notify_backup \"sh /etc/postgresql/telegram.sh \\'BACKUP pghost195.etagi.com получил VIP\\'\"\\n notify_fault \"sh /etc/postgresql/telegram.sh \\'FAULT pghost195.etagi.com получил VIP\\'\"\\n\\n}\\n \\r\\nРестартим \\r\\n/etc/init.d/keepalived restart \\r\\nНастраиваем keepalived на 2-ой ноде(pghost196) nano /etc/keepalived/keepalived.conf ! this is who emails will go to on alerts\\n notification_email {\\n admin@domain.com\\n\\n ! add a few more email addresses here if you would like\\n }\\n notification_email_from servers@domain.com\\n ! I use the local machine to relay mail\\n smtp_server smt.local.domain\\n smtp_connect_timeout 30\\n ! each load balancer should have a different ID\\n ! this will be used in SMTP alerts, so you should make\\n ! each router easily identifiable\\n lvs_id LVS_HAPROXY-pghost196\\n}\\n\\n}\\nvrrp_instance haproxy-pghost196 {\\n interface eth0\\n state MASTER\\n virtual_router_id 192\\n priority 80\\n ! send an alert when this instance changes state from MASTER to BACKUP\\n smtp_alert\\n authentication {\\n auth_type PASS\\n auth_pass passwordforcluster\\n }\\n track_script {\\n chk_http_port\\n }\\n virtual_ipaddress {\\n 10.1.1.192/32 dev eth0\\n }\\n notify_master \"sh /etc/postgresql/telegram.sh \\'MASTER pghost196.etagi.com получил VIP\\'\" \\n notify_backup \"sh /etc/postgresql/telegram.sh \\'BACKUP pghost196.etagi.com получил VIP\\'\"\\n notify_fault \"sh /etc/postgresql/telegram.sh \\'FAULT pghost196.etagi.com получил VIP\\'\"\\n\\n}\\n \\r\\nНастраиваем keepalived на 3-ой ноде(pghost197) nano /etc/keepalived/keepalived.conf ! this is who emails will go to on alerts\\n notification_email {\\n admin@domain.com\\n\\n ! add a few more email addresses here if you would like\\n }\\n notification_email_from servers@domain.com\\n ! I use the local machine to relay mail\\n smtp_server smt.local.domain\\n smtp_connect_timeout 30\\n ! each load balancer should have a different ID\\n ! this will be used in SMTP alerts, so you should make\\n ! each router easily identifiable\\n lvs_id LVS_HAPROXY-pghost197\\n}\\n\\n}\\nvrrp_instance haproxy-pghost197 {\\n interface eth0\\n state MASTER\\n virtual_router_id 192\\n priority 50\\n ! send an alert when this instance changes state from MASTER to BACKUP\\n smtp_alert\\n authentication {\\n auth_type PASS\\n auth_pass passwordforcluster\\n }\\n track_script {\\n chk_http_port\\n }\\n virtual_ipaddress {\\n 10.1.1.192/32 dev eth0\\n }\\n notify_master \"sh /etc/postgresql/telegram.sh \\'MASTER pghost197.etagi.com получил VIP\\'\" \\n notify_backup \"sh /etc/postgresql/telegram.sh \\'BACKUP pghost197.etagi.com получил VIP\\'\"\\n notify_fault \"sh /etc/postgresql/telegram.sh \\'FAULT pghost197.etagi.com получил VIP\\'\"\\n\\n}\\n \\r\\nРестартим Скрытый текст /etc/init.d/keepalived restart\\n \\r\\nКак мы видим, мы также можем использовать скрипты, например для уведомления при изменении состояния. Смотрим следующую секцию Скрытый текст notify_master \"sh /etc/postgresql/telegram.sh \\'MASTER pghost195.etagi.com получил VIP\\'\" \\n notify_backup \"sh /etc/postgresql/telegram.sh \\'BACKUP pghost195.etagi.com получил VIP\\'\"\\n notify_fault \"sh /etc/postgresql/telegram.sh \\'FAULT pghost195.etagi.com получил VIP\\'\"\\n \\r\\nТак же из конфига видно что мы настроили VIP на 10.1.8.111 который будет жить на eth0. В случае падения ноды pghost195 он перейдет на pghost196, т.е. подключение мы так же будем настраивать через IP 10.1.1.192. так же установим на pghost197, только изменим vrrp_instance и lvs_id LVS_. \\r\\nНа нодах pghost196,pghost197 отключим keepalived. Он будет запускаться только после процедуры failover promote, которая описана в файле. Мы указали Скрытый текст promote_command=\\'sh /etc/postgresql/failover_promote.sh\\'\\nfollow_command=\\'sh /etc/postgresql/failover_follow.sh\\'\\n \\r\\nв файле /etc/repmgr.conf (см. в конфигах выше). \\r\\nДанные скрипты будут запускаться при возникновении failover ситуации -отказе мастера. \\r\\npromote_command=\\'sh /etc/postgresql/failover_promote.sh — выпоняет номинированный на master host, \\r\\nfollow_command=\\'sh /etc/postgresql/failover_follow.sh\\' — исполняют ноды, которые следуют за мастером. \\r\\nКонфиги promote_command=\\'sh /etc/postgresql/failover_promote.sh\\' #!/bin/bash\\nCLHOSTS=\"pghost195 pghost196 pghost197 pghost198 pghost205 \"\\nrepmgr standby promote -f /etc/repmgr.conf;\\necho \"Отправка оповещений\";\\nsh /etc/postgresql/failover_notify_master.sh;\\necho \"Выводим список необходимых хостов в файл\"\\nrepmgr -f /etc/repmgr.conf cluster show | grep node | awk \\' {print $7} \\' | sed \"s/host=//g\" | sed \\'/port/d\\' &gt; /etc/postgresql/cluster_hosts.list\\nrepmgr -f /etc/repmgr.conf cluster show | grep FAILED | awk \\' {print $6} \\' | sed \"s/host=//g\" | sed \"s/&gt;//g\" &gt; /etc/postgresql/failed_host.list\\nrepmgr -f /etc/repmgr.conf cluster show | grep master | awk \\' {print $7} \\' | sed \"s/host=//g\" | sed \"s/&gt;//g\" &gt; /etc/postgresql/current_master.list\\nrepmgr -f /etc/repmgr.conf cluster show | grep standby | awk \\' {print $7} \\' | sed \"s/host=//g\" | sed \\'/port/d\\' &gt; /etc/postgresql/standby_host.list\\n\\n####КОПИРУЮ ИНФО ФАЙЛЫ И ФАЙЛЫ-ТРИГГЕРЫ НА ДРУГИЕ НОДЫ КЛАСТЕРА#####################\\nfor CLHOST in $CLHOSTS\\ndo\\nrsync -arvzSH --include \"*.list\" --exclude \"*\" /etc/\\\\postgresql/ postgres@$CLHOST:/etc/postgresql/\\ndone\\n\\necho \"Начинаю процедуру восстановления упавшего сервера,если не триггера /etc/postgresql/disabled\"\\n\\nfor FH in $(cat /etc/postgresql/failed_host.list)\\ndo\\nssh postgres@$FH &lt;&lt;OFF\\nsh /etc/postgresql/register.sh;\\necho \"Рестартуем repmgrd на других нодах\"\\nsh /etc/postgresql/repmgrd.sh;\\nsh /etc/postgresql/failover_notify_restoring_ended.sh;\\nOFF\\ndone\\n\\n\\necho \"Стопаем repmgrd на ноде, ставшей мастером\"\\npkill repmgrd\\n\\necho \"Работаем с Keepalived\"\\n\\n follow_command=\\'sh /etc/postgresql/failover_follow.sh\\' repmgr standby follow -f /etc/repmgr.conf;\\necho \"Отправка оповещений\";\\nsh /etc/postgresql/failover_notify_standby.sh;\\npkill repmgrd;\\nrepmgrd -f /etc/repmgr.conf -p /var/run/postgresql/repmgrd.pid -m -d -v &gt;&gt; /var/log/postgresql/repmgr.log 2&gt;&amp;1;\\n \\r\\nСкрипт остановки мастера — принудительного failover, удобно использовать для тестирования процедур «перевыборов» в кластере. follow_command=\\'sh /etc/postgresql/stop_master.sh\\' #!/bin/bash \\r\\nrepmgr -f /etc/repmgr.conf cluster show | grep master | awk \\' {print $7} \\' | sed «s/host=//g» | sed «s/&gt;//g» &gt; /etc/postgresql/current_master.list \\r\\nfor CURMASTER in $(cat /etc/postgresql/current_master.list) \\r\\ndo \\r\\nssh postgres@$CURMASTER &lt;&lt;OFF \\r\\ncd ~/9.6; \\r\\n/usr/lib/postgresql/9.6/bin/pg_ctl -D /etc/postgresql/9.6/main -m immediate stop; \\r\\ntouch /etc/postgresql/disabled; \\r\\nOFF \\r\\nsh /etc/postgresql/telegram.sh «ТЕКУЩИЙ МАСТЕР ОСТАНОВЛЕН» \\r\\ndone \\r\\nС помощью скриптов можно понять логику работу и настроить сценарии под себя. Как мы видим из кода, нам будет необходим доступ к root пользователю от пользователя postgres. Получаем его таким же образом — через ключи. Доступ к root от postgres su postgres\\nssh-copy-id -i ~/.ssh/id_rsa.pub root@pghost195\\nssh-copy-id -i ~/.ssh/id_rsa.pub root@pghost196\\nssh-copy-id -i ~/.ssh/id_rsa.pub root@pghost197\\nssh-copy-id -i ~/.ssh/id_rsa.pub root@pghost198\\nssh-copy-id -i ~/.ssh/id_rsa.pub root@pghost205\\n \\r\\nПовторяем на всех нодах. Для особых параноиков, можем настроить скрипт проверки состояний и добавить его в крон например раз в 2 минуты. Сделать это можно без, используя конструкции и используя полученные значения из файлов. Скрытый текст repmgr -f /etc/repmgr.conf cluster show | grep node | awk \\' {print $7} \\' | sed \"s/host=//g\" | sed \\'/port/d\\' &gt; /etc/postgresql/cluster_hosts.list\\nrepmgr -f /etc/repmgr.conf cluster show | grep FAILED | awk \\' {print $6} \\' | sed \"s/host=//g\" | sed \"s/&gt;//g\" &gt; /etc/postgresql/failed_host.list\\nrepmgr -f /etc/repmgr.conf cluster show | grep master | awk \\' {print $7} \\' | sed \"s/host=//g\" | sed \"s/&gt;//g\" &gt; /etc/postgresql/current_master.list\\nrepmgr -f /etc/repmgr.conf cluster show | grep standby | awk \\' {print $7} \\' | sed \"s/host=//g\" | sed \\'/port/d\\' &gt; /etc/postgresql/standby_host.list\\n Дополнения и устранение неисправностей. Сбор статистики запросов в базу \\r\\nМы добавили библиотеку pg_stat_statements( необходимо сделать рестарт) Скрытый текст su postgres\\ncd ~\\npg_ctl restart;\\n \\r\\nДалее активируем расширение: Скрытый текст # CREATE EXTENSION pg_stat_statements;\\n \\r\\nПример собранной статистики: Скрытый текст # SELECT query, calls, total_time, rows, 100.0 * shared_blks_hit /\\n nullif(shared_blks_hit + shared_blks_read, 0) AS hit_percent\\n FROM pg_stat_statements ORDER BY total_time DESC LIMIT 10;\\n \\r\\nДля сброса статистики есть команда pg_stat_statements_reset: Скрытый текст # SELECT pg_stat_statements_reset();\\n Удаление ноды из кластера если она ‘FAILED’ Скрытый текст DELETE FROM repmgr_etagi_test.repl_nodes WHERE name = \\'node1\\';\\n \\r\\nгде — etagi_test — название кластера; \\r\\nnode1 — имя ноды в кластере Проверка состояния репликации Скрытый текст plsq\\n#SELECT EXTRACT(EPOCH FROM (now() - pg_last_xact_replay_timestamp()))::INT;\\n\\n 00:00:31.445829\\n(1 строка)\\n \\r\\nЕсли в базе давно не было Insert’ов — то это значение будет увеличиваться. На hiload базах это значение будет стремиться к нулю. Устранение ошибки Slot \\'repmgr_slot_номер слота\\' already exists as an active slot \\r\\nОстанавливаем postgresql на той ноде, на которой возникла ошибка Скрытый текст su postgres\\npg_ctl stop;\\n \\r\\nНа ноде master\\'e Скрытый текст su postgres \\npsql repmgr\\n#select pg_drop_replication_slot(\\'repmgr_slot_4\\');\\n Устранение ошибки ОШИБКА: база данных «dbname» занята другими пользователями \\r\\nДля того чтобы удалить базу данных на мастере необходимо отключить всех пользователей, спользующих данную базу а затем удалить ее. Скрытый текст plsq\\n# SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE datname = \\'dbname\\';\\n# DROP DATABASE dbname;\\n Устранение ошибки INSERT или UPDATE в таблице «repl_nodes» нарушает ограничение внешнего ключа ОШИБКА: INSERT или UPDATE в таблице «repl_nodes» нарушает ограничение внешнего ключа «repl_nodes_upstream_node_id_fkey». DETAIL: Ключ (upstream_node_id)=(-1) отсутствует в таблице «repl_nodes». \\r\\nЕсли у вас возникла данная ошибка при попытке ввести упавшую ноду обратно в кластер то необходимо провести процедуру switchover любой ноды в кластере(standby) Скрытый текст repmgr -f /etc/repmgr.conf standby switchover\\n \\r\\nStandby станет мастером. На “Старом Мастере” ставшем standby Скрытый текст repmgr -f /etc/repmgr.conf standby follow\\n Ошибка ВАЖНО: не удалось открыть каталог \"/var/run/postgresql/9.6-main.pg_stat_tmp\": \\r\\nПросто создаем каталог Скрытый текст su postgres\\nmkdir -p /var/run/postgresql/9.6-main.pg_stat_tmp\\n Устранение ошибки при регистрации кластера no password supplied. \\r\\nПри регистрации кластера после того как мы слили с ноды данные бывает возникает ошибка \\r\\n“no password supplied”. Не стали с ней долго разбираться, помогла перезагрузка, видимо какой-то сервис не смог нормально загрузиться. Backup кластера Бэкап кластера делается командой pg_dumpall | gzip -c &gt; filename.gz\\n Скрипт бэкапа баз данных Postgres backup_pg.sh #!/bin/bash\\nDBNAMES=\"db1 db2 db3\" \\nDATE_Y=`/bin/date \\'+%y\\'`\\nDATE_M=`/bin/date \\'+%m\\'`\\nDATE_D=`/bin/date \\'+%d\\'`\\nSERVICE=\"pgdump\"\\n#DB_NAME=\"repmgr\";\\n\\n#`psql -l | awk \\'{print $1}\\' `\\nfor DB_NAME in $DBNAMES\\ndo\\n\\techo \"CREATING DIR /Backup/20${DATE_Y}/${DATE_M}/${DATE_D}/${DB_NAME} \"\\n\\tBACKUP_DIR=\"/Backup/20${DATE_Y}/${DATE_M}/${DATE_D}/${DB_NAME}\"\\n\\tmkdir -p $BACKUP_DIR;\\n\\t\\tpg_dump -Fc --verbose ${DB_NAME} | gzip &gt; $BACKUP_DIR/${DB_NAME}.gz\\n\\t\\t# Делаем dump базы без даты, для того что дальше извлечь их нее функции\\n\\t\\tpg_dump -Fc -s -f $BACKUP_DIR/${DB_NAME}_only_shema ${DB_NAME} \\n\\t\\t/bin/sleep 2;\\n\\t\\t# Создаем список функция\\n\\t\\tpg_restore -l $BACKUP_DIR/${DB_NAME}_only_shema | grep FUNCTION &gt; $BACKUP_DIR/function_list\\n\\t\\tdone\\n\\n\\n\\n##Как восстановить функции\\n\\n#########################\\n#pg_restore -h localhost -U username -d имя_базы -L function_list db_dump\\n########################\\n\\n\\n### КАК ВОССТАНОВИТЬ ОДНУ ТАБЛИЦУ ИЗ БЭКАПА, например таблицу payment.\\n#pg_restore --dbname db1 --table=table1 имядампаБД\\n####ЕСЛИ ЖЕ ВЫ ХОТИТЕ СЛИТЬ ТАБЛИЦУ В ПУСТУЮ БАЗУ, ТО НЕОБХОДИМО ВОССОЗДАТЬ СТРУКТУРУ БД\\n###pg_restore --dbname ldb1 имядампаБД_only_shema\\n Заключение \\r\\nИтак, что мы получили в итоге: \\r\\n — кластер master-standby из четырех нод; \\r\\n — автоматический failover в случае отказа мастера(с помощью repmgr’a); \\r\\n — балансировку нагрузки(на чтение) через haproxy и pgbouncer(менеджер сеансов); \\r\\n — отсутствие единой точки отказа — keepalived переносит ip адрес на другую ноду, которая была автоматически “повышена” до мастера в случае отказа; \\r\\n — процедура восстановления(возвращение отказавшего сервера в кластер) не является трудоемкой — если разобраться); \\r\\n — гибкость системы — repmgr позволяет настроить и другие события в случае наступления инцидента с помощью bash скриптов; \\r\\n — возможность настроить систему “под себя”. \\r\\nДля начинающего специалиста настройка данной схемы может показаться немного сложной, на практике же, один раз стоит со всем хорошо разобраться и вы сможете создать HA системы на базе Postgresql и сами управлять сценариями реализации механизма Failover.',\n 'title': 'Кластер высокой доступности на postgresql 9.6 + repmgr + pgbouncer + haproxy + keepalived + контроль через telegram',\n 'summary': '\\r\\nНа сегодняшний день процедура реализации «failover» в Postgresql является одной из самых простых и интуитивно понятных. Для ее реализации необходимо определиться со сценариями файловера — это...',\n 'url': 'https://habrahabr.ru/company/etagi/blog/314000/',\n 'keywords': ['postgresq',\n  'haproxy',\n  'pgbouncer',\n  'keepalived',\n  'repmgr',\n  'cluster',\n  'HA',\n  'failover',\n  'replication']}"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "habr[2]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "urls_habr = [x['url'] for x in habr]\n",
    "summaries_habr = [x['summary'] for x in habr]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/paul/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/paul/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/paul/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/3990 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "201b86e4cd59479284c887d150002351"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "('MassTransit это open source библиотека, разработанная на языке C# для .NET платформы, упрощающая работу с шиной данных, которая используется при построении распределенных приложений и реализации...',\n 'masstransit это open source библиотека разработать язык c net платформа упрощать работа шина данные который использоваться построение распределить приложение реализация ...')"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segmenter = Segmenter()\n",
    "morph_vocab = MorphVocab()\n",
    "emb = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "\n",
    "def natasha_lemmatize(text):\n",
    "    text = text.lower()\n",
    "    try:\n",
    "        text = LatexNodes2Text().latex_to_text(text)\n",
    "    except:\n",
    "        text = text.lower()\n",
    "    doc = Doc(text)\n",
    "    doc.segment(segmenter)\n",
    "    doc.tag_morph(morph_tagger)\n",
    "    for token in doc.tokens:\n",
    "        token.lemmatize(morph_vocab)\n",
    "    return [_.lemma for _ in doc.tokens]\n",
    "\n",
    "def cleaner(txt, noise):\n",
    "    res = []\n",
    "    for word in txt:\n",
    "        if word not in noise:\n",
    "            res.append(word)\n",
    "    return ' '.join(res)\n",
    "\n",
    "def prep(txt_list):\n",
    "    noise = stopwords.words('russian') + list(punctuation) + ['что', 'то', 'кто', 'привет', 'весь', 'всем', 'какой', 'ваш', 'внимание', 'добрый', 'время', 'сатья', 'данный', 'хотеть', 'рассказать', 'посвятить', 'результат', 'результаты', 'представить', 'привести']\n",
    "    for i in tqdm(range(len(txt_list))):\n",
    "        txt_list[i] = cleaner(natasha_lemmatize(txt_list[i]), noise)\n",
    "\n",
    "\n",
    "\n",
    "cleaned_summaries_habr = summaries_habr.copy()\n",
    "prep(cleaned_summaries_habr)\n",
    "summaries_habr[0], cleaned_summaries_habr[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "4072"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jsonlines\n",
    "cyberlink = []\n",
    "for fl in [f'ru_data/cyberleninka_{i}.jsonlines' for i in range(5)]:\n",
    "    with jsonlines.open(fl, 'r') as jsonl_f:\n",
    "        cyberlink += [obj for obj in jsonl_f]\n",
    "len(cyberlink)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "extracting abstracts:   0%|          | 0/4072 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "93058e43bf254133a2f4938a428c68a2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def split_lang(txt):\n",
    "    rus = ''\n",
    "    eng = ''\n",
    "    for sent in txt.split('.'):\n",
    "        try:\n",
    "            if detect(sent) == 'ru':\n",
    "                rus += sent + '.'\n",
    "            else:\n",
    "                eng += sent + '.'\n",
    "        except:\n",
    "            continue\n",
    "    return rus, eng\n",
    "\n",
    "abstract_rus_link = []\n",
    "abstract_eng_link = []\n",
    "url_cyber_leninka = []\n",
    "\n",
    "for sup in tqdm(cyberlink, desc='extracting abstracts'):\n",
    "    abstr = sup['abstract']\n",
    "    r, e = split_lang(abstr)\n",
    "    abstract_rus_link.append(r)\n",
    "    abstract_eng_link.append(e)\n",
    "    url_cyber_leninka.append(sup['url'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/4072 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a81265a0c5704e0bb6b3888993b6a6d5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "('Изложен метод проектирования устройств подачи нитки верхнего петлителя и определения рационального процесса ее потребления на основании использования диаграммы согласования функций подачи и потребления нитки верхнего петлителя при образовании трехниточного краеобметочного стежка 504 типа. Алгоритм, представленный для расчета диаграммы потребления нитки верхнего петлителя с учетом вероятных вариантов процесса взаимодействия рабочих органов, материала и нитки верхнего петлителя с ниткой нижнего петлителя, в совокупности с математическим аппаратом, используемый при этом, позволяет автоматизировать одну из особо сложных стадий в проектировании краеобметочной машины. Данный метод проектирования нитеподачи нитки верхнего петлителя будет полезен для разработчиков и эксплуатационников .',\n 'изложить метод проектирование устройство подача нитка верхний петлитель определение рациональный процесс потребление основание использование диаграмма согласование функция подача потребление нитка верхний петлитель образование трехниточный краеобметочный стежок 504 тип алгоритм расчет диаграмма потребление нитка верхний петлитель учет вероятный вариант процесс взаимодействие рабочий орган материал нитка верхний петлитель нитка нижний петлитель совокупность математический аппарат использовать это позволять автоматизировать особо сложный стадия проектирование краеобметочный машина метод проектирование нитеподача нитка верхний петлитель полезный разработчик эксплуатационник')"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_abstract_r_link = abstract_rus_link.copy()\n",
    "prep(cleaned_abstract_r_link)\n",
    "abstract_rus_link[0], cleaned_abstract_r_link[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "''"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstract_rus_link[2]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "data_comb = np.array(cleaned_abstract_r_link + cleaned_summaries_habr)\n",
    "data_unck_comb = abstract_rus_link + summaries_habr\n",
    "url_comb = url_cyber_leninka + urls_habr"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "''"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_comb[2]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "(8062, 8062)"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(url_comb), len(data_comb)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "df_rus = pd.DataFrame({'data_clean' : data_comb, 'data_unclean' : data_unck_comb, 'url' : url_comb})\n",
    "df_rus.to_csv('habr_cyberleninka.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data_comb, np.arange(data_comb.shape[0]), test_size=0.3,\n",
    "                                                    random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_name = 'paraphrase-multilingual-mpnet-base-v2'\n",
    "sentence_model = SentenceTransformer(model_name, device=\"cuda\")\n",
    "study = optuna.create_study(sampler=optuna.samplers.CmaEsSampler(), direction=\"maximize\")\n",
    "study.enqueue_trial(\n",
    "    {\n",
    "        'top_n_words': 10,\n",
    "        'n_gram_range': 1,\n",
    "        'min_topic_size': 10,\n",
    "        'min_dist': 0.000001,\n",
    "        'n_neighbors': 15,\n",
    "        'n_components': 5,\n",
    "        # 'cluster_selection_epsilon': 0.0,\n",
    "        # 'min_cluster_size': 5,\n",
    "        'min_samples': 5\n",
    "    }\n",
    ")\n",
    "study.optimize(objective, n_trials=50, n_jobs=1)\n",
    "params_ = extract_pars(study.best_trial.params)\n",
    "\n",
    "# sentence_model = SentenceTransformer(model_name, device=\"cuda\")\n",
    "umap_model = umap.UMAP(**params_[0], random_state=42)\n",
    "hdbscan_model = hdbscan.HDBSCAN(**params_[1])\n",
    "topic_model = BERTopic(embedding_model=sentence_model, umap_model=umap_model, hdbscan_model=hdbscan_model, **params_[2])\n",
    "\n",
    "topics = topic_model.fit_transform(X_test)\n",
    "compute_metrics(topics, topic_model, X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "umap_model = umap.UMAP(**params_[0], random_state=42)\n",
    "hdbscan_model = hdbscan.HDBSCAN(**params_[1], prediction_data = True)\n",
    "topic_model = BERTopic(embedding_model=sentence_model, umap_model=umap_model, hdbscan_model=hdbscan_model,\n",
    "                       **params_[2], verbose=True, calculate_probabilities=True)\n",
    "\n",
    "topics = topic_model.fit_transform(data_comb)\n",
    "compute_metrics(topics, topic_model, data_comb)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# topic_model.save('best_rus')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(topic_model.get_topics())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cv = []\n",
    "npmi  = []\n",
    "td = []\n",
    "n_tops = list(range(100, 20, -10))\n",
    "for nr in tqdm(range(100, 20, -10)):\n",
    "    umap_model = umap.UMAP(**params_[0], random_state=42)\n",
    "    hdbscan_model = hdbscan.HDBSCAN(**params_[1], prediction_data = True)\n",
    "    topic_model = BERTopic(embedding_model=sentence_model, umap_model=umap_model, hdbscan_model=hdbscan_model,\n",
    "                           **params_[2], verbose=True, calculate_probabilities=True)\n",
    "\n",
    "    topics = topic_model.fit_transform(data_comb)\n",
    "    new_topics, new_probs = topic_model.reduce_topics(data_comb, topics[0], topics[1],\n",
    "                                                      nr_topics=nr)\n",
    "    td_, npmi_, cv_ = compute_metrics((new_topics, new_probs), topic_model, data_comb)\n",
    "    cv.append(cv_)\n",
    "    td.append(td_)\n",
    "    npmi.append(npmi_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "\n",
    "fig = px.line(x=n_tops, y=cv)\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig = px.line(x=n_tops, y=npmi)\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig = px.line(x=n_tops, y=td)\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "best_nr = 70\n",
    "umap_model = umap.UMAP(**params_[0], random_state=42)\n",
    "hdbscan_model = hdbscan.HDBSCAN(**params_[1], prediction_data = True)\n",
    "topic_model = BERTopic(embedding_model=sentence_model, umap_model=umap_model, hdbscan_model=hdbscan_model,\n",
    "                       **params_[2], verbose=True, calculate_probabilities=True)\n",
    "\n",
    "topics = topic_model.fit_transform(data_comb)\n",
    "new_topics, new_probs = topic_model.reduce_topics(data_comb, topics[0], topics[1],\n",
    "                                                  nr_topics=best_nr)\n",
    "\n",
    "df = pd.DataFrame({'uncleaned texts' : data_unck_comb, 'texts' : data_comb, 'urls' : url_comb})\n",
    "df['topics'] = new_topics\n",
    "df.to_csv('best_so_far.csv')\n",
    "topic_model.save('best_model_so_far')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "topic_model.visualize_topics()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "topic_model.visualize_distribution(new_probs[200], min_probability=0.015)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "topic_model.visualize_hierarchy(top_n_topics=50)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "topic_model.visualize_barchart(top_n_topics=8)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "topic_model.visualize_term_rank()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}